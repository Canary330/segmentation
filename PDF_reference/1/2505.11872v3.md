## Page 1

PRS-Med: Position Reasoning Segmentation with
Vision-Language Model in Medical Imaging
Quoc-Huy Trinh1,3,6
Minh-Van Nguyen2,6
Jun Zeng5
Ulas Bagci3
Debesh Jha4
1Aalto University
2Denmark Technical University
3Northwestern University
4 University of South Dakota
5 Chongqing University of Posts and Telecommunications
6 SpexAI GmbH
huy.trinh@aalto.fi, s242503@dtu.dk, zeng.cqupt@gmail.com
ulas.bagci@northwestern.edu, debesh.jha@usd.edu
Figure 1: Example interaction between the doctor and proposed assistant model. Given a natural
language prompt about a tumour and its location, along with an input image, the assistant returns a
segmentation result and a spatially grounded textual description.
Abstract
Recent advancements in prompt-based medical image segmentation have enabled
clinicians to identify tumors using simple input like bounding boxes or text prompts.
However, existing methods face challenges when doctors need to interact through
natural language or when position reasoning is required - understanding spatial
relationships between anatomical structures and pathologies. We present PRS-Med,
a framework that integrates vision-language models with segmentation capabilities
to generate both accurate segmentation masks and corresponding spatial reasoning
outputs. Additionally, we introduce the MMRS dataset (Multimodal Medical in
Positional Reasoning Segmentation), which provides diverse, spatially-grounded
question-answer pairs to address the lack of position reasoning data in medical
imaging. PRS-Med demonstrates superior performance across six imaging modali-
ties (CT, MRI, X-ray, ultrasound, endoscopy, RGB), significantly outperforming
state-of-the-art methods in both segmentation accuracy and position reasoning.
Our approach enables intuitive doctor-system interaction through natural language,
facilitating more efficient diagnoses. Our dataset pipeline, model, and codebase
will be released to foster further research in spatially-aware multimodal reasoning
for medical applications.
1
Introduction
In medical settings, doctors typically perform diagnoses by observing potential tumour locations
to assess tissue conditions. This makes the position reasoning and segmentation essential tasks for
supporting early and accurate diagnosis. In the age of the medical assistant agents, several Vision
Preprint. Under review.
arXiv:2505.11872v3  [cs.CV]  15 Aug 2025



### 图片

![2505.11872v3_page1_img1.png](images/2505.11872v3_page1_img1.png)

## Page 2

Large Language Models (VLMs), including LLaVA-Med [27], Med-MoE [23], HuatuoGPT [7],
and MedVLM-R1 [36] have been proposed to assist in tumour detection from medical imaging
and provide reasoning about the tumours. Although these models have demonstrated promising
capabilities in reasoning and accurately identifying tumours within medical images, there is a practical
challenge arises as doctors often seek to identify unknown tumour locations through implicit
questions or conversational interactions. Moreover, during the diagnosis process, the result that
doctors need to predict the status of the anatomy is the location of the tumour in the medical images,
not only the information about the tumour. This is the reason why position reasoning segmentation
becomes critical to help the doctor in recognizing the tumours, thereby facilitating more effective
diagnosis and treatment. Additionally, this technique can help the clinics to create an automated
screening system, thus reducing the cost of doing it manually.
Recently, in the natural image domain, the challenge of reasoning for segmentation has been addressed
by several works such as LISA [26], LLM-SEG [48], and SEGLLM [52]. These models have
achieved notable success in enhancing object reasoning and accurately identifying object positions
via segmentation, and provide simple reasoning about the object. However, these models do not
explicitly address position reasoning and are not well-trained on medical imaging data. Thus,
applying this model to medical imaging faces several difficulties.
Inspired by recent methods like SpatialVLM [6], SpatialRGPT [9], and LocVLM [38], which
demonstrate the promising initial results in position understanding and reasoning, we aim to address
the current challenge in developing an intelligent medical image assistant. In this paper, we propose
PRS-Med: Position Reasoning Segmentation through Vision-Language Models for medical imaging.
This method aims to perform position-reasoning segmentation from the simple question or command
input, via the Multimodal-LLM (Vision Language Model) that incorporates the vision feature from
the SAM image encoder. The model can describe the position of the tumour in text, recognize and
point out the tumours inside the medical image through the segmentation mask. As a result, PRS-Med
is an intelligent assistant that can interactively answer questions from the doctor and visually indicate
the tumour position in an interpretable manner.
In summary, our contributions are in four folds:
• We propose the PRS-Med, a novel position reasoning model that integrates multimodal
vision-language learning with a lightweight TinySAM image encoder to perform spatially-
aware tumour segmentation via the implicit natural language prompt.
• We construct and release the Multimodal Medical in Positional Reasoning Segmentation
(MMRS) dataset pipeline, which can construct a comprehensive positioning benchmark
pipeline, designed to generate diverse, spatially grounded question-answer pairs and address
the lack of datasets and evaluation tools for position reasoning in the field of medical imaging
domain.
• We conduct extensive quantitative and qualitative experiments across six modalities (CT,
MRI, X-ray, ultrasound, endoscopy, and RGB), demonstrating PRS-Med surpasses state-of-
the-art (SOTA) segmentation and reasoning models in both accuracy and generalization.
• We open-source the dataset pipeline, model, and codebase to the community, which facili-
tates the development of the spatially-aware multimodal LLMs in medical imaging.
The rest of this paper is organized as follows: in Section 2, we briefly review existing methods related
to this research. Then we introduce position reasoning and the segmentation dataset preparation
pipeline in Section 3. Afterward, we introduce our proposed model, PRS-Med, in Section 4, and the
position reasoning benchmark for Medical Imaging in Section 5. Experiment setups are in Section 6.
Results of the model assessment is mentioned in Section 7 Finally, we present the conclusion in
Section 8.
2
Related Work
Reasoning Image Segmentation: Recent advancements in reasoning segmentation have begun to
integrate high-level reasoning, particularly through the use of the Multimodal-LLMs. Notable works
in the domain include LISA [26], LLM-Seg [49], and SegLLM [53], which include a special [SEG]
token used to extract segmentation-specific embeddings from the model. However, applying this
2


## Page 3

method for the position reasoning is challenging due to the limitations in the annotated data and
the difficulty of adapting to medical-specific vocabulary. To address this limitation, in our design
of PRS-Med, we integrate the global token embeddings from the Multimodal-LLM with the visual
features of medical images, enabling effective position-aware segmentation.
Medical Image Segmentation: Traditional Medical image segmentation has long relied on fully
supervised CNN-based architectures like U-Net [39] and its variant such as ResUNet++ [22],
nnu-net [19], DoubleUNet [20], TransResUNet [45], and Swin-UNet [5]. More recently, sev-
eral promptable segmentations have emerged as a response to the growing demand for interactive
and context-aware medical AI. MedSAM [33], SAM-Med2D [12] adapts the Segment Anything
Model (SAM) [24] to medical settings, supporting box- and point-prompted segmentation. However,
MedSAM and SAM-Med2D still lack semantic understanding of positional cues within free-form
text. In contrast, BiomedParse [58] directly uses text prompts to infer object shapes and positions,
learning implicit position priors. Despite its novelty, BiomedParse does not couple segmentation
with contextual reasoning, nor does it support implicit or conversational prompts beyond class names.
PRS-Med integrates position reasoning with segmentation, enabling an interactive framework that
responds to contextual questions and generates both position reasoning outputs and corresponding
segmentation masks.
Multimodal-LLM in Medical Imaging: Multimodal large language models (MLLMs) have recently
shown promising results in medical image reasoning tasks. Prominent methods such as Med-
Flamingo [35], Med-MoE [35], GSCo [17], HuatuoGPT [7] and MedVLM-R1 [36], are built upon
vision and Language models like LLaVA [30], Qwen2-VL [50], and Multimodal Llama [46] through
the training technique via visual instruction tuning or reinforcement learning methods. Despite their
promising results, these models struggle with medical image segmentation, which is a critical task
for accurate disease diagnosis. Moreover, they also inherit spatial reasoning limitations from their
Multimodal-LLMs which is observed in the prior works such as SpatialVLM [6], Loc-VLM [38],
and Spatialrgpt [9]. To address these challenges, we propose PRS-Med, which extends MLLM
capabilities by enabling accurate segmentation and enhancing position reasoning for medical imaging
applications.
3
Position Reasoning Dataset Preparation
The research community has proposed several methods for constructing datasets to support multi-
modal reasoning, such as LAION [40], and LLaVA [30], which rely on either human annotators or
large language models (LLMs) for labeling. However, these approaches face significant challenges
when applied to the medical domain, particularly in positional reasoning segmentation, due to the
limited availability of high-quality specialized datasets and the critical requirement for accuracy.
Accuracy is especially critical in healthcare, where incorrect information can have serious conse-
quences. Moreover, manual annotation is both time-consuming and costly, making it difficult to
scale. To address these issues, we propose a novel method that leverages the logical capabilities
and text generation power of LLMs. Specifically, the LLM is used to generate templated sentences
that incorporate positional information extracted directly from the ground truth segmentation masks.
This approach enhances dataset diversity, reduces hallucination during pseudo-data generation, and
lowers the overall cost by minimizing the need for repeated API calls or hosting LLMs. Figure 2
illustrates the flow of our dataset creation pipeline to serve the positional reasoning segmentation task
of medical imaging.
Our dataset creation is including two stages, the first stage is Dictionary preparation, where we
prepare the template for the question and answer, and the second stage is the Data creation, where
we will do the mapping and combine the position, modality information to the template.
Regarding the first stage, we prepare samples of the questions and answers as following be the
template for the dictionary generation:
1. Question: Where is the tumour position in the <image type> medical image?
2. Answer: There is a <tumour name> in the <position>
To begin, we leverage a large language model—specifically ChatGPT—to generate 50 question-and-
answer templates based on the mentioned question-answer pair for training and 5 for testing. These
templates are carefully crafted using detailed prompts to ensure that, when combined with the tumour
name and positional information, the resulting sentences are coherent and contextually appropriate.
3


## Page 4

Figure 2: Position reasoning dataset question-answer pair creation consists of two stages: (1)
dictionary creation, which stores question–answer templates; and (2) data generation, which produces
question–answer pairs using positional information extracted from segmentation masks.
In the second stage, we extract positional information from the segmentation mask. Given a binary
mask Xmask, we first derive the bounding box {x, y, w, h}, representing the location of the tumour
within the image. From this, we calculate the center point of the tumour as xcenter = {x + w
2 , y + h
2 }.
Next, we divide the image into four quadrants—top left, top right, bottom left, and bottom right—as
illustrated in Figure 2. Based on the location of xcenter, we determine which quadrant the tumour
lies in and assign it a corresponding label. In addition to handling cases where tumours are located
near the image center, we also compute the distance between xcenter and the geometric center of the
image. If this distance falls below a predefined threshold, we label the tumour as being near the center.
Finally, we integrate the extracted positional information with the question-and-answer templates
to generate the final dataset of spatially grounded tumour descriptions. As a result, of this dataset
pipeline, we release the pseudo-position reasoning segmentation dataset, which is used for training
the Position Reasoning Medical Image segmentation.
4
PRS-Med
Overall Architecture: The primary goal of PRS-Med is to perform position reasoning segmentation,
enabling the model to explain the location of tumours in an image along with relevant medical
information. Additionally, the segmentation head allows the model to perform tumour segmentation
within the image using a single prompt. The overall architecture is illustrated in Figure 3. Different
with the reasoning segmentation of the natural image as LISA [26], in our work, we leverage the full
token embedding sequence instead of add a special token [SEG] due to the limitations of the training
data, which make the model difficult to learn the new vocabulary.
Figure 3: The architecture of PRS-Med comprises three primary components: (1) the Tiny Vision
Backbone, (2) the Prompt Mask Decoder, and (3) the Multimodal-LLM. The framework accepts
two input modalities: an image and a text-based prompt (e.g., a question). The image is processed
through a vision encoder, while the prompt is embedded via a LoRA-adapted Multimodal-LLM. The
fused representations are used to produce two outputs: a segmentation mask for the tumor regions,
and a textual description specifying the tumor’s location.
This framework consists of four main modules. The first is the Vision-Language Model; in our
case, we use LLaVA-Med [27], as it is a well-trained Multimodal-LLM for the medical dataset,
4



### 图片

![2505.11872v3_page4_img1.png](images/2505.11872v3_page4_img1.png)

![2505.11872v3_page4_img2.png](images/2505.11872v3_page4_img2.png)

## Page 5

which we can benefit from its domain of expertise. The second module is the Tiny SAM image
encoder, employed from TinySAM [41], which is used to encode the input image. The third module
is the Decoder, which includes a fusion component that combines image features from the SAM
image encoder with the vision-language embeddings from the Medical Vision-Language Model to
generate the final segmentation mask. In addition, we include a Language Model Head to perform
the reasoning task.
During training, due to the challenges of fine-tuning the full LLaVA-Med model, we apply Low-Rank
Adaptation (LoRA) [18] to enable the model to effectively learn position reasoning information from
our prepared dataset.
Vision Backbone: The primary objective of the vision backbone is to extract pixel-level features
from medical images to support conditional segmentation. For this purpose, we adopt the image
encoder from TinySAM [41], which is based on the lightweight TinyViT architecture [56]. This
design enables efficient image encoding while reducing computational resource requirements.
Given a batch of b input images Ximage ∈Rb×3×W ×H, the images are processed through a tiny
vision transformer model Fvis, consisting of approximately four transformer layers, to produce an
image representation embedding zimage ∈Rb×256× W
16 × H
16 . This encoder extracts dense visual fea-
tures zimage, which are used for the segmentation task. During training, the encoder is kept unfrozen
to allow it to adapt to the medical image domain, thereby improving segmentation performance. The
reason for the design choice of this TinyViT-based vision backbone is detailed in Section 6.
By leveraging pre-trained weights from the TinySAM image encoder, our model can better adapt
to the medical domain without initializing weights from scratch, which contributes to improved
segmentation outcomes.
Multimodal-LLM: Most current Multimodal-LLM backbones applied to the medical domain—such
as Flamingo [2], LLaVA [30], Qwen-VL [51], and InternVL [8]—demonstrate strong reasoning
capabilities. However, they generally lack the ability to generate masks for visual recognition tasks
and struggle to comprehend positional information, such as the position of objects within an image.
Notably, embeddings from the final layer of these multimodal LLMs have proven highly valuable
in various robotics applications, as demonstrated in works like TinyVLA [55], RoboMamba [31],
and Groot-N1 [4]. Inspired by these insights, we adopt a multimodal LLM design in this module to
serve both as a feature extractor for conditioning the masked decoder and as a component for position
reasoning. Different with LISA [26] (the reasoning segmentation approach in the natural image),
when they create a new token id for segmentation, in this work, we propose a method that leverages
directly the joint embedding from the Multimodal-LLM, which can reduce the computation cost in
the training for adapting the new vocabulary set.
To generate the Multimodal-LLM embedding zemb ∈Rb×l×4096 (where l is the token length),
and the reasoning output ztxt ∈Rb×l from the input image Ximage ∈Rb×3×w×h and input
text Xtxt ∈Rb×l×d (where d is the vocabulary size), we define Fmllm as a parametric function
implemented using LLaVA-Med [27]. The complete process of the model can be described as follows:
zemb = Fmllm(Ximage, Xtxt)
(1)
ztxt = p(ztxt|Ximage, Xtxt) =
lY
i=1
pθ(zi
txt|Ximage, Xi−1
txt ) with 1 < i < l
(2)
where θ is the trainable parameter. In our case, θ is from the parameter of the parametric function
Fmllm, which is the weight of the LLaVA-med model.
During training, due to the high computational cost associated with fully supervised fine-tuning of
the LLaVA-Med model, we employ the LoRA method [18] as an adapter. This approach allows
the model to learn reasoning capabilities from the our generated position medical reasoning dataset
while adapting to generate meaningful embeddings for the mask decoder. The selection of LoRA
hyperparameters is detailed in Section 6.
Prompt Mask Decoder: The goal of this module is to predict the segmented mask from two inputs
are medical images representation feature zimage and the embedded image-text prompt zemb from
the Multimodal-LLM. For more detailed visualization of the overall decoder, it is shown in Figure 5
of AppendixA.5.
5


## Page 6

In the Fusion module, given the image representation from the vision encoder, denoted as zimage ∈
Rb×256×16×16, and the conditioning input from the Multimodal LLM, denoted as zemb ∈Rb×l×4096,
the overall fusion process is formalized in Equation 3 and Equation 4.
zfused = SA(σ(Fproj
θ1
(zimage)Fproj
θ2
(zemb)T )
√dk
)Fproj
θ2
(zemb))
(3)
zfused = zfused + zimage
(4)
where dk is the scaling value, SA(.) is the Self-Attention layer, and σ(.) is the softmax function.
First, the image representation zimage is reshaped to a new form zimage ∈Rb×(16×16)×256 to enable
interaction with the embedding zemb ∈Rb×l×4096 from the Multimodal LLM. As shown in Equation 3,
two projection layers, Fproj
θ1
and Fproj
θ2 , are applied to project both features into a shared latent space
of dimension 256. This alignment allows effective fusion through a cross-attention mechanism,
which integrates the image features with the multimodal embeddings. The choice of cross-attention
is motivated by the dynamic length of the zemb sequences, making it a more flexible and suitable
alternative to simple addition or concatenation. Following the fusion, a self-attention layer is employed
to model the internal dependencies within the target sequence. The resulting fused representation,
zfused ∈Rb×(16×16)×d, is then reshaped to Rb×256×16×16. Finally, as described in Equation 4, a skip
connection is introduced to preserve gradient flow and mitigate the vanishing gradient problem during
training.
In the Mask Prediction module, the input zfused is passed through a stack of transposed 2D convolu-
tional layers, each followed by Batch Normalization and ReLU activation. This series of operations
progressively upsamples zfused to produce the final segmentation output zmask ∈Rb×1×1024×1024.
Objective Function: This model is trained end-to-end by using the segmentation loss (Lseg) and
text generation loss Ltext. The overall objective function is depicted in Equation 5.
L = λsegLseg + λtxtLtext
(5)
where λseg and λtxt shows the importance of each loss in the overall framework.
Regarding Lseg, we employ a combination of Binary Cross-Entropy and Dice loss [43], which is a
common choice in image segmentation tasks. For Ltxt, we use the Categorical Cross-Entropy (CE)
loss. Let ˆymask denote the ground truth mask and zmask the predicted mask; similarly, let ˆytxt be the
ground truth token index sequence and ztxt the predicted text logits. Equations 6 and 7 illustrate the
formulations of the aforementioned loss functions.
Lseg = LBCE(ˆymask, zmask) + Ldice(ˆymask, zmask)
(6)
Ltext = LCE(ˆytxt, ztxt)
(7)
By employing this objective function, PRS-Med can simultaneously learn position reasoning while
also learn to perform segmentation. Notably, training the segmentation task alongside position
reasoning allows the segmentation loss to serve as feedback for the position reasoning component,
thereby enhancing the overall performance of the position reasoning task.
5
Position Reasoning Benchmark
Position reasoning is a challenging task, as positional content can be represented in various ways.
This variability makes the benchmark difficult to solve using purely logic-based methods, so it is
necessary for the human evaluation to verify the results, which is costly. To address this challenge,
we propose a benchmark method based on an ensemble of agents. Specifically, we leverage two large
language models—Qwen 3 [57], LLaMA 3.1 [16] to evaluate the predicted answers by reasoning
about positional content. Our assumption for this benchmark that the Large Language Model has a
strong ability to compare the context meaning of two sentences, so if we can leverage this ability to
make it compare the predicted sentence with the ground-truth, we can automate the benchmark on
a large scale of data. Moreover, the ensemble results can reduce the bias of 1 model on the overall
result.
6


## Page 7

In our benchmark setup, we define a set of two agents A = {a1, a2}. For each agent, we design three
different prompts that follow the chain-of-thought [54] technique and instruct the model to return
in the yes/no format. These prompts are applied to each pair (Pi, Gi) in the set B, prompting the
model to determine whether the predicted answer is similar to the ground truth. Each "yes" answer
is awarded one point, while "no" answers receive zero. The prompts used in the benchmark are
described in Appendix A.3. The overall result for each agent is calculated by taking the average and
getting the standard deviation. The overall result is calculated by the mean of the accuracy results
from two agents
By using this ensemble-based approach, we reduce the bias introduced by relying on a single LLM,
as done in previous works, which often results in a high risk of benchmark bias.
6
Experimental Setup
Dataset: As mentioned in Section 3, our training dataset is constructed by combining several medical
data sources images with generated question-answer annotations for 6 different types of images are
ultrasound, MRI, RGB image, CT Image, Xray, and endoscopy images as these are the popular
image types, which is mentioned by Biomedparse [59]. All of the datapoints are collected BUSI [1],
BrainMRI [10, 11], ISIC [14], LungCT [25], LungXray [13, 25], Kvasir-SEG [21], and ClinicDB
[3]. For the train and test split, we follow the original split from the dataset source to ensure the fair
comparison. Furthermore, to increase the difficulty and better evaluate generalization, particularly for
polyp tissue segmentation, we augment the test set with additional unseen data from CVC300 [47],
ETIS [42], and ColonDB [44], alongside the test splits from Kvasir-SEG and ClinicDB. This strategy
allows for a more rigorous assessment of our method’s generalization performance. More detail of
the dataset is discussed in the appendix.
Implementation Details: In our implementation, experiments were conducted using an H100 80GB
GPU. The model was trained for 20 epochs, requiring approximately 24 GPU hours with a batch size
of 8. We employed the AdamW optimizer with a learning rate of 1 × 10−4, and the best checkpoint
was selected at epoch 18 based on validation performance. For the LoRA hyperparameter of the
Multimodal-LLM, we set the rank of LoRA is 16, alpha value as the same, with the dropout value is
0.05. The LoRA weight is initialized following a uniform distribution.
Comparison Baseline: To compare our work with SOTA methods, we do the comparison on two
tasks are the segmentation task and the position reasoning task. For the Segmentation task, we
compare our methods with the Foundation Segmentation model of medical imaging, such as SAM-
Med 2D [60] (2024), and Biomedparse [59] (2024), and the reasoning segmentation model, which
is reproduced on our dataset is LISA [26] with two versions are 7B and 13 B. Regarding the SAM
model in medical imaging, there is a challenge that most medical segmentation model is based on
the box prompt. For this reason, we leverage the Grounding Dino [32] as the text understanding
model to extract the boxes coordinate for the segmentation task. In the Reasoning task, due to lack of
method done reasoning segmentation, we reproduce the fine-tuning process on our dataset for the
Multimodal-LLM for medical image, which inclues LLaVA-Med [27] (2024), HuatuoGPT-Vision [7]
(2024), and MedVLM-R1 [36] (2025) to do the reasoning benchmark. In all of the comparisons, we
try to reproduce the best-practice hyperparameter for each method for the fairest comparison.
Evaluation Metric: For the evaluation, we use the mDice, and mIoU to benchmark the segmentation
results, as the standard of the medical segmentation task. In terms of the reasoning task, we evaluate
through the accuracy metric.
7
Experimental Result
7.1
Qualitative results
Segmentation Results To evaluate the overall performance of our method against SOTA segmentation
approaches, we compare it with several prior works. Table 1 presents results on radiology images
of different tissue types, specifically Breast and Brain, and Lung-related radiology images in both
CT-Scans and X-rays. Table 2 demonstrates the performance of our method on endoscopic and RGB
images for polyp detection and skin cancer segmentation, respectively.
7


## Page 8

Method
Breast Ultrasound
Brain MRI
Lung CT-Scan
Lung X-ray
mDice ↑
mIoU ↑
mDice ↑
mIoU ↑
mDice ↑
mIoU ↑
mDice ↑
mIoU ↑
G-Dino + SAM-Med2D [34]
0.515
0.441
0.667
0.625
0.540
0.392
0.401
0.300
Biomedparse [59]
0.783
0.698
0.294
0.245
0.088
0.061
0.972
0.949
LISA-7B [26]
0.299
0.246
0.478
0.402
0.478
0.402
0.179
0.122
LISA-13B [26]
0.526
0.441
0.124
0.076
0.656
0.528
0.664
0.535
PRS-Med
0.817
0.729
0.803
0.757
0.968
0.943
0.969
0.942
vs previous works
+0.034
+0.031
+0.136
+0.132
+0.428
+0.551
-0.003
-0.007
Table 1: Qualitative results of PRS-Med on the radiology images across four tissue types. The
highest score in each column is shown in bold, and the second highest is underlined.
Method
Polyp Endoscopy
Skin RGB Image
mDice ↑
mIoU ↑
mDice ↑
mIoU ↑
G-Dino + SAM-Med2D [34]
0.488
0.418
0.237
0.171
Biomedparse [59]
0.824
0.774
0.924
0.867
LISA-7B [26]
241
0.202
0.464
0.368
LISA-13B (Llama 2 based) [26]
0.312
0.247
0.643
0.536
PRS-Med
0.843
0.791
0.875
0.799
vs previous works
+0.019
+0.017
-0.049
-0.068
Table 2: Qualitative results of PRS-Med on endoscopy and RGB skin lesion image. The highest
score in each column is shown in bold, and the second highest is underlined.
As shown in the quantitative results, our method outperforms the second-best model, LISA-13B,
across four different modalities: Breast Ultrasound, Lung CT-Scan, Lung X-ray, Polyp Endoscopy
superior performance in both Dice score and mIoU. Although there is a slight performance gap in
the Brain MRI and Skin RGB Image modality, our method still demonstrates strong generalization
across a wide range of medical imaging tasks, which can demonstrate the effectiveness of our method
in solving this problem.
Comparison with the methods based on the Grounding DINO combined with Med-SAM, our model
shows the improvement due to the better attribution to the use of a Multimodal LLM, which more
effectively represents user queries in a joint embedding space. This enhances the quality of the
embeddings and boosts segmentation performance through stronger reasoning capabilities.
In terms of the comparison with other SOTA reasoning-based segmentation methods, our approach
achieves better results thanks to the domain expertise embedded in our Multimodal LLM. This
domain adaptation improves the representation of medical image features, leading to more accurate
segmentation. While LISA-7B and LISA-13B are based on larger, more extensively pre-trained
Multimodal LLMs, their limited understanding of medical-specific visual features results in weaker
performance in this domain.
Position Reasoning results To assess the performance of the PRS-Med, we do the evaluation on the
position reasoning accuracy with SOTA methods in the Multimodal-LLM for medical images, which
is depicted in Table 3. As the illustrated results, our method surpasses previous Multimodal-LLM
model in the position reasoning task. These results indicate that our Multimodal-LLM model, which
also has the ability in doing segmentation can help it improve its position reasoning ability.
Method
Qwen Benchmark
Llama 3.1 Benchmark
Final result
LLaVA-Med [27]
0.426 (± 0.006)
0.412 (± 0.008)
0.419
HuatuoGPT-Vision [7]
0.450 (± 0.001)
0.424 (± 0.003)
0.437
MedVLM-R1 [36]
0.217(± 0.002)
0.295 (± 0.007)
0.256
PRS-Med
0.509 (± 0.007)
0.556 (± 0.004)
0.533
vs previous works
+0.059
+0.132
+0.096
Table 3: Qualitative results of PRS-Med on position reasoning with SOTA multimodal LLMs.
Qualitative analysis In Figure 4 of Appendix A.4, we present qualitative visualizations that highlight
the improvements achieved by PRS-Med. The results clearly show that PRS-Med outperforms all
previous methods in segmentation performance, consistently generating complete masks with the
lowest loss. These improvements are attributed to the informative feature extraction enabled by our
lightweight vision encoder, as well as the effectiveness of the fusion module. Overall, the results
provide strong evidence for the promise of our approach.
8


## Page 9

7.2
Ablation Study
To assess the choice of the module in our framework, we conduct several experiments to assess the
performance and the limitations of each module. The experiments are conducted in the same training
and testing dataset with the benchmark. Regarding the metrics, we calculate the average mDice
(Avg-mDice) and average mIoU (Avg-mIoU) of the results on different modalities in our test dataset
to have the best assessment of the generalization of each choice.
The choice of Vision Encoder backbone: In this experiment, we evaluate different designs of the
Vision Encoder, as described in Section 4, with the results presented in Table 4. Among the evaluated
models, Med-SAM with trainable parameters achieves the highest performance, with an Avg-mDice
improvement of +0.01 and an Avg-mIoU improvement of +0.006% compared to the second-best
encoder, Tiny-SAM. However, this performance gain comes at a significant computation cost, since
Med-SAM has over nine times more trainable parameters than Tiny-SAM, resulting in substantially
higher computational demands. Due to this trade-off, we selected Tiny-SAM as the optimal choice.
It provides a favorable balance between performance and efficiency, making it more suitable given
our computational constraints.
Vision Backbone
Parameters
Avg-mDice ↑
Avg-mIoU ↑
Med-SAM (frozen)
21.52M
0.788
0.714
Med-SAM (trainable)
292.60M
0.889
0.833
Tiny-SAM (frozen)
21.73M
0.735
0.659
Tiny-SAM (trainable)
31.49M
0.879
0.827
Table 4: Comparison results of the different choices of the vision encoder backbones.
The choice of MLLM Backbone To evaluate the choice of MLLM backbone for PRS-Med, we
conducted experiments comparing three models are LLaVA-1.5 [29, 28], LLaVA-1.6 [29, 28] and
LLaVA-Med—using the same 7B backbone and fine-tuned via the LoRA approach. The comparison
focuses on two tasks: segmentation and position reasoning, as shown in Table 5. The results indicate
that the overall performance of the LLaVA-Med baseline surpasses that of LLaVA-1.5 and LLaVA-1.6.
This improvement can be attributed to LLaVA-Med’s enhanced adaptation to the medical domain,
which enables it to better handle tasks involving medical data.
MLLM
Parameters
Avg-mDice ↑
Avg-mIoU ↑
Reasoning Accuracy ↑
LLaVA-1.5 (LoRA)
34.63M
0.709
0.642
0.414
LLaVA-1.6 (LoRA)
31.49M
0.744
0.671
0.509
LLaVA-Med (LoRA)
31.49M
0.879
0.827
0.533
Table 5: Comparison of the different choices of the Multimodal-LLM models.
Effect of segmentation module on the position reasoning of the model: To evaluate the impact
of the segmentation module on the overall framework’s position reasoning capability, we conduct
a series of experiments under three different settings: (1) using the frozen Llava-Med model, (2)
applying LoRA training only to the Llava-Med model, and (3) utilizing our full pipeline with the
integrated segmentation module. According to the results from our benchmark, the frozen Llava-Med
model achieves an accuracy of 0.278, while the LoRA-trained Llava-Med model improves to 0.419.
In comparison, our approach, which incorporates the segmentation module, achieves the highest
accuracy of 0.533, demonstrating that training with the segmentation task significantly enhances the
model’s position reasoning performance, and the Llava-Med original model does not do well in the
position reasoning.
8
Conclusion
We introduced PRS-Med, a novel framework for position reasoning segmentation in medical imaging
that integrates a lightweight image encoder with a vision-language model to perform spatially-aware
tumor segmentation via natural language prompts. Our approach enables intuitive interaction with
medical images through conversational queries while providing accurate segmentation masks and
position reasoning. The MMRS dataset generation pipeline addresses the critical lack of positional
reasoning data in the medical domain. Our comprehensive evaluation across six imaging modalities
demonstrates significant improvements over state-of-the-art methods in both segmentation accuracy
and position reasoning capabilities. By releasing our dataset pipeline, model, and codebase, we aim to
accelerate research in spatially-aware multimodal reasoning for medical applications. PRS-Med has
the potential to enhance clinical workflows by improving diagnostic accuracy, reducing interpretation
time, and enabling more intuitive interaction between physicians and AI systems.
9


## Page 10

References
[1] Walid Al-Dhabyani, Mohammed Gomaa, Hussien Khaled, and Aly Fahmy. Dataset of breast
ultrasound images. Data in brief, 28:104863, 2020.
[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,
Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual
language model for few-shot learning. Advances in neural information processing systems,
35:23716–23736, 2022.
[3] Jorge Bernal, F. Javier Sánchez, Gloria Fernández-Esparrach, Debora Gil, Cristina Rodríguez,
and Fernando Vilariño. WM-DOVA maps for accurate polyp highlighting in colonoscopy:
Validation vs. saliency maps from physicians. CMIG, pages 99–111, 2015.
[4] Johan Bjorck, Fernando Castañeda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan,
Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al. Gr00t n1: An open foundation model
for generalist humanoid robots. arXiv preprint arXiv:2503.14734, 2025.
[5] Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian, and Manning
Wang. Swin-unet: Unet-like pure transformer for medical image segmentation. In European
conference on computer vision, pages 205–218, 2022.
[6] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei
Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
14455–14465, 2024.
[7] Junying Chen, Chi Gui, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen,
Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, et al. Huatuogpt-vision, towards injecting
medical visual knowledge into multimodal llms at scale. arXiv preprint arXiv:2406.19280,
2024.
[8] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong
Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning
for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition, pages 24185–24198, 2024.
[9] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong
Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision language models. arXiv
preprint arXiv:2406.01584, 2024.
[10] Jun Cheng, Wei Huang, Shuangliang Cao, Ru Yang, Wei Yang, Zhaoqiang Yun, Zhijian Wang,
and Qianjin Feng. Enhanced performance of brain tumor classification via tumor region
augmentation and partition. PloS one, 10(10):e0140381, 2015.
[11] Jun Cheng, Wei Yang, Meiyan Huang, Wei Huang, Jun Jiang, Yujia Zhou, Ru Yang, Jie Zhao,
Yanqiu Feng, Qianjin Feng, et al. Retrieval of brain tumors by adaptive spatial pooling and
fisher vector representation. PloS one, 11(6):e0157112, 2016.
[12] Junlong Cheng, Jin Ye, Zhongying Deng, Jianpin Chen, Tianbin Li, Haoyu Wang, Yanzhou Su,
Ziyan Huang, Jilong Chen, Lei Jiangand Hui Sun, Junjun He, Shaoting Zhang, Min Zhu, and
Yu Qiao. Sam-med2d, 2023.
[13] Muhammad EH Chowdhury, Tawsifur Rahman, Amith Khandakar, Rashid Mazhar, Muham-
mad Abdul Kadir, Zaid Bin Mahbub, Khandakar Reajul Islam, Muhammad Salman Khan, Atif
Iqbal, Nasser Al Emadi, et al. Can ai help in screening viral and covid-19 pneumonia? Ieee
Access, 8:132665–132676, 2020.
[14] Noel CF Codella, David Gutman, M Emre Celebi, Brian Helba, Michael A Marchetti, Stephen W
Dusza, Aadi Kalloo, Konstantinos Liopyris, Nabin Mishra, Harald Kittler, et al. Skin lesion
analysis toward melanoma detection: A challenge at the 2017 international symposium on
biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic). In 2018
IEEE 15th international symposium on biomedical imaging (ISBI 2018), pages 168–172, 2018.
[15] Nick DiSanto. Isic melanoma dataset, 2023.
[16] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian,
Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama
3 herd of models. arXiv preprint arXiv:2407.21783, 2024.
10


## Page 11

[17] Sunan He, Yuxiang Nie, Hongmei Wang, Shu Yang, Yihui Wang, Zhiyuan Cai, Zhixuan Chen,
Yingxue Xu, Luyang Luo, Huiling Xiang, et al. Gsco: Towards generalizable ai in medicine via
generalist-specialist collaboration. arXiv preprint arXiv:2404.15127, 2024.
[18] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR,
1(2):3, 2022.
[19] Fabian Isensee, Jens Petersen, Andre Klein, David Zimmerer, Paul F Jaeger, Simon Kohl, Jakob
Wasserthal, Gregor Koehler, Tobias Norajitra, Sebastian Wirkert, et al. nnu-net: Self-adapting
framework for u-net-based medical image segmentation. arXiv preprint arXiv:1809.10486,
2018.
[20] Debesh Jha, Michael A Riegler, Dag Johansen, Pål Halvorsen, and Håvard D Johansen. Doubleu-
net: A deep convolutional neural network for medical image segmentation. In 2020 IEEE 33rd
International symposium on computer-based medical systems (CBMS), pages 558–564, 2020.
[21] Debesh Jha, Pia H Smedsrud, Michael A Riegler, Pål Halvorsen, Thomas de Lange, Dag
Johansen, and Håvard D Johansen. Kvasir-SEG: A Segmented Polyp Dataset. In Multimedia
Modeling, 2020.
[22] Debesh Jha, Pia H Smedsrud, Michael A Riegler, Dag Johansen, Thomas De Lange, Pål
Halvorsen, and Håvard D Johansen. Resunet++: An advanced architecture for medical image
segmentation. In Proceedings of the 2019 IEEE International Symposium on Multimedia (ISM),
pages 225–2255, 2019.
[23] Songtao Jiang, Tuo Zheng, Yan Zhang, Yeying Jin, Li Yuan, and Zuozhu Liu. Med-moe:
Mixture of domain-specific experts for lightweight medical vision-language models. arXiv
preprint arXiv:2404.10237, 2024.
[24] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,
Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In
Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4015–4026,
2023.
[25] D. Konya. CT lung, heart, and trachea segmentation. 2020.
[26] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa:
Reasoning segmentation via large language model. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 9579–9589, 2024.
[27] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan
Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision
assistant for biomedicine in one day. Advances in Neural Information Processing Systems,
36:28541–28564, 2023.
[28] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual
instruction tuning, 2023.
[29] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee.
Llava-next: Improved reasoning, ocr, and world knowledge, January 2024.
[30] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances
in neural information processing systems, 36:34892–34916, 2023.
[31] Jiaming Liu, Mengzhen Liu, Zhenyu Wang, Pengju An, Xiaoqi Li, Kaichen Zhou, Senqiao
Yang, Renrui Zhang, Yandong Guo, and Shanghang Zhang. Robomamba: Efficient vision-
language-action model for robotic reasoning and manipulation. Advances in Neural Information
Processing Systems, 37:40085–40110, 2024.
[32] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan
Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training
for open-set object detection. In European Conference on Computer Vision, pages 38–55, 2024.
[33] Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and Bo Wang. Segment anything in
medical images. Nature Communications, 15:654, 2024.
[34] Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and Bo Wang. Segment anything in
medical images. Nature Communications, 15(1):654, 2024.
11


## Page 12

[35] Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Yash Dalmia, Jure Leskovec,
Cyril Zakka, Eduardo Pontes Reis, and Pranav Rajpurkar. Med-flamingo: a multimodal medical
few-shot learner. In Machine Learning for Health (ML4H), pages 353–367, 2023.
[36] Jiazhen Pan, Che Liu, Junde Wu, Fenglin Liu, Jiayuan Zhu, Hongwei Bran Li, Chen Chen,
Cheng Ouyang, and Daniel Rueckert. Medvlm-r1: Incentivizing medical reasoning capability
of vision-language models (vlms) via reinforcement learning. arXiv preprint arXiv:2502.19634,
2025.
[37] Tawsifur Rahman, Amith Khandakar, Yazan Qiblawey, Anas Tahir, Serkan Kiranyaz, Saad
Bin Abul Kashem, Mohammad Tariqul Islam, Somaya Al Maadeed, Susu M Zughaier, Muham-
mad Salman Khan, et al. Exploring the effect of image enhancement techniques on covid-19
detection using chest x-ray images. Computers in biology and medicine, 132:104319, 2021.
[38] Kanchana Ranasinghe, Satya Narayan Shukla, Omid Poursaeed, Michael S Ryoo, and Tsung-Yu
Lin. Learning to localize objects improves spatial reasoning in visual-llms. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12977–12987,
2024.
[39] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
U-net: Convolutional networks
for biomedical image segmentation. In Medical image computing and computer-assisted
intervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5-9,
2015, proceedings, part III 18, pages 234–241, 2015.
[40] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton
Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open
dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.
[41] Han Shu, Wenshuo Li, Yehui Tang, Yiman Zhang, Yihao Chen, Houqiang Li, Yunhe Wang,
and Xinghao Chen. Tinysam: Pushing the envelope for efficient segment anything model. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 20470–20478,
2025.
[42] Juan S. Silva, Aymeric Histace, Olivier Romain, Xavier Dray, and Bertrand Granado. Towards
embedded detection of polyps in WCE images for early diagnosis of colorectal cancer. IJCARS,
pages 283–293, 2014.
[43] Carole H Sudre, Wenqi Li, Tom Vercauteren, Sebastien Ourselin, and M Jorge Cardoso. Gener-
alised dice overlap as a deep learning loss function for highly unbalanced segmentations. In
Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision
Support: Third International Workshop, DLMIA 2017, and 7th International Workshop, ML-
CDS 2017, Held in Conjunction with MICCAI 2017, Québec City, QC, Canada, September 14,
Proceedings 3, pages 240–248, 2017.
[44] Nima Tajbakhsh, Suryakanth R. Gurudu, and Jianming Liang. Automated Polyp Detection in
Colonoscopy Videos Using Shape and Context Information. TMI, pages 630–644, 2016.
[45] Nikhil Kumar Tomar, Annie Shergill, Brandon Rieders, Ulas Bagci, and Debesh Jha. Transresu-
net: Transformer based resu-net for real-time colonoscopy polyp segmentation. arXiv preprint
arXiv:2206.08985, 2022.
[46] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-
thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open
and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
[47] David Vázquez, Jorge Bernal, Francisco Javier Sánchez, Glòria Fernández-Esparrach, Anto-
nio M. López, Adriana Romero, Michal Drozdzal, and Aaron C. Courville. A Benchmark for
Endoluminal Scene Segmentation of Colonoscopy Images. Journal of Healthcare Engineering,
2017.
[48] Junchi Wang and Lei Ke. Llm-seg: Bridging image segmentation and large language model
reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 1765–1774, 2024.
[49] Junchi Wang and Lei Ke. Llm-seg: Bridging image segmentation and large language model
reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 1765–1774, 2024.
12


## Page 13

[50] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing
Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language model’s perception
of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024.
[51] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing
Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language model’s perception
of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024.
[52] XuDong Wang, Shaolun Zhang, Shufan Li, Konstantinos Kallidromitis, Kehan Li, Yusuke
Kato, Kazuki Kozuka, and Trevor Darrell. Segllm: Multi-round reasoning segmentation. arXiv
preprint arXiv:2410.18923, 2024.
[53] XuDong Wang, Shaolun Zhang, Shufan Li, Konstantinos Kallidromitis, Kehan Li, Yusuke
Kato, Kazuki Kozuka, and Trevor Darrell. Segllm: Multi-round reasoning segmentation. arXiv
preprint arXiv:2410.18923, 2024.
[54] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,
Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.
Advances in neural information processing systems, 35:24824–24837, 2022.
[55] Junjie Wen, Yichen Zhu, Jinming Li, Minjie Zhu, Zhibin Tang, Kun Wu, Zhiyuan Xu, Ning Liu,
Ran Cheng, Chaomin Shen, et al. Tinyvla: Towards fast, data-efficient vision-language-action
models for robotic manipulation. IEEE Robotics and Automation Letters, 2025.
[56] Kan Wu, Jinnian Zhang, Houwen Peng, Mengchen Liu, Bin Xiao, Jianlong Fu, and Lu Yuan.
Tinyvit: Fast pretraining distillation for small vision transformers. In European conference on
computer vision, pages 68–85, 2022.
[57] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan
Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint
arXiv:2412.15115, 2024.
[58] Theodore Zhao, Yu Gu, Jianwei Yang, Naoto Usuyama, Ho Hin Lee, Tristan Naumann, Jianfeng
Gao, Angela Crabtree, Jacob Abel, Christine Moung-Wen, et al. Biomedparse: a biomedical
foundation model for image parsing of everything everywhere all at once. arXiv preprint
arXiv:2405.12971, 2024.
[59] Theodore Zhao, Yu Gu, Jianwei Yang, Naoto Usuyama, Ho Hin Lee, Tristan Naumann, Jianfeng
Gao, Angela Crabtree, Jacob Abel, Christine Moung-Wen, et al. Biomedparse: a biomedical
foundation model for image parsing of everything everywhere all at once. arXiv preprint
arXiv:2405.12971, 2024.
[60] Jiayuan Zhu, Abdullah Hamdi, Yunli Qi, Yueming Jin, and Junde Wu. Medical sam 2: Segment
medical images as video via segment anything model 2. arXiv preprint arXiv:2408.00874,
2024.
13


## Page 14

A
Appendix
A.1
Broader Impact
The broader impact of PRS-Med lies in its capability for position reasoning segmentation. As
an intelligent assistant, it can support doctors in rapid screening and efficiently gathering detailed
information about a patient’s disease status. This, in turn, helps reduce diagnosis and treatment time,
enabling more patients to receive care within the same day.
A.2
Detail of the LLM Usage
We prompt GPT-4 models for generating the template of questions and answer as we mentioned in
Section 3. The detail of the prompt and the template is mentioned in Section 3. Besides that, we use
public LLM models in our benchmark methods, as the LLama-3 [46] model and the Qwen-3 [57].
A.3
Sample of the evaluation prompts
As mentioned in Section 5, in this section, we will include the benchmark prompt for the Large
Language Model to do the benchmark. As mentioned in Section 5, each model will run three times
on these different questions, then for each model, we calculate the average and the standard deviation
to get the benchmark results for each agent. Then the final result is calculated by the results of the
agents, which means that with more agents, we can reduce the bias of the benchmark from the LLM.
As the LLM also has hallucinations, it can affect to the benchmark results. However, as our
observation, the hallucination is minor, which create a tiny effect, not affecting considerably the
judging process for the overall performance of the models.
The following list shows the different templates of the benchmark prompts:
1. As a medical image special list
Instruction: Answer the question related to the position content, return only yes or no
Prompt: Given the following question and answer with the ground truth, is the position in
the answer similar or same with the ground truth and match with the question. Sample -
Question: {question} | groundtruth: {groundtruth} | Prediction: {answer} Return yes
if they are similar. Return no if they are different.
2. As a doctor
Instruction: Answer the question related to the position content, return only yes or no
Prompt: Check if the location information provided in the prediction aligns with the position
mentioned in the ground truth and is relevant to the question. Input — Question: question |
Ground Truth: groundtruth | Prediction: answer Respond with Yes if the positions are
similar. Respond with No if they are different.
3. As you are a doctor and you are looking to the medical image:
Instruction: Answer the question related to the position content, return only yes or no
Prompt: Evaluate whether the predicted answer captures the same or similar positional con-
text as the ground truth, based on the provided question. Question: question Groundtruth:
groundtruth Prediction: answer Answer with "Yes" if the position is similar, otherwise
"No"
A.4
Qualitative analysis
Figure 4 shows the inference results of the segmentation between our method and previous works.
This visualization is mentioned in Section 6 of the paper.
14


## Page 15

Figure 4: Qualitative comparison between PRS-Med in the segmentation results with previous
methods.
In this qualitative benchmark, we compare six different types of medical images, each with five
anatomical specifications. A total of five methods are evaluated and visualized for qualitative
segmentation performance. As shown in the results, PRS-Med—our proposed method—achieves the
best overall qualitative performance. However, in the case of polyp endoscopy images, BiomedParse
produces slightly better results due to its stronger adaptation to boundary contexts, which are critical
for accurate segmentation in endoscopic images. Despite this limitation, our method demonstrates
competitive generalization compared to previous approaches, highlighting its promising potential for
robust medical image segmentation.
A.5
Overall Decoder design
Figure 5 visualizes our overall design for our decoder model, which is mentioned in Section 4.
Figure 5: Overall design of the Prompt Mask Decoder block, including two modules are fusion module
to fuse the medical image representation and the conditioning embedding from the Multimodal-LLM;
the other is the mask prediction to predict the segment mask.
15



### 图片

![2505.11872v3_page15_img1.png](images/2505.11872v3_page15_img1.png)

![2505.11872v3_page15_img2.png](images/2505.11872v3_page15_img2.png)

## Page 16

A.6
Explanation for the Position Identification
In Section 3, we divide the image into 4 quadrants in order to identify the position of the tumour in
the image, as the representation in Figure 6.
Figure 6: Examples of the position identification for dataset creation.
For images containing a single tumor, we identify the tumor’s position by selecting the region that
consists the largest mask area. In the case of images with two tumors, we detect both regions and
combine their positional descriptions into a single sentence. Additionally, we compute the Euclidean
distance between the center of the image and the centroid of the mask to determine whether a tumor
is located near the center. In our pipeline, we set the distance threshold for being considered "near
the center" at 20 unit.
For example, in the first image shown in Figure 6, the tumor is located in the top-left region and near
the center, while the second image contains two tumors identified in the bottom-left and bottom-right
regions, which both tumours are also near the center. The final output sentence describing the tumor
positions is as follows:
• There is a tumour in the top left region and near the center.
• A tumour is located in the bottom left quadrant, near the center, and another(or in other
words in our prompt: ”, ” and, with) tumour is located in the bottom right quadrant, near the
center.
By using this logic-based approach, we can infer the position of the tumor and map it to the
corresponding position vocabulary in our dictionary. This mapped position can then be integrated into
a predefined template to generate the final question-answer pairs. The advantage of this method is
that it eliminates the need for manual labeling of question-answer pairs, thereby reducing annotation
costs and minimizing human error.
A.7
Dataset Information
In our work, we employ several different datasets from the open-source datasets, including BUSI [1]
dataset, ISIC [14, 15], Kvasir-SEG [21], ClinicDB [3], ColonDB [44], CVC300 [47], ETIS-Polyric
[42], LungCT [25], Lung Xray [13, 37], and Brain MRI dataset [10, 11]. These dataset focus on
six different types of images are ultrasound image, RGB image, endoscopy image, CT-scan image,
Lung Xray image, and MRI image. These are the common medical image types in the real-world
application, mentioned in BiomedParse [59]. In total, we have 28650 images and question-answer
pairs for training, and 4647 images along with the question-answer pairs for testing. Due to the
limitations of the number of medical images, this dataset is reasonable for the experiments in our
paper, and the results shows that our methods can adapt well in this dataset setting.
16



### 图片

![2505.11872v3_page16_img1.png](images/2505.11872v3_page16_img1.png)

## Page 17

Image and specification type
Number of train sample
Number of test sample
Breast Ultrasound
599
113
Skin RBG Image
900
379
Polyp Endoscopy
1450
798
Lung CT
7959
1800
Lung Xray
16280
957
Brain MRI
1462
600
Table 6: Statistics for the number of training and test samples for each specification and type of
images.
Regarding the number of question-answer pairs, 50 templates are used for the training, and 5
templates for the testing.
A.8
Question-Answer Dataset creation
Prompt to generate the question-answer pair: For the generation of the question-answer pair,
we leverage the chain-of-thought [54] technique. The prompt for the generation is mentioned as
following.
1. As a doctor, and you have to work with medical image everyday
2. Instruction: Given the Question and answer template as example:
Q: Where is the tumour position in the <image type> medical image? A: There is a <tumour
name> in the <position>.
3. Prompt: Generate the informative question and answer pair that keep the template of the
provided template.
Template for question and answer pairs generated:
For the training dataset, we generate 50
question-answer pairs templates. For facilitating the analysis , we show only ten examples of the
templates in our training data:
1. Q: In the provided {image_type} scan, where can the tumour be observed?
A: The tumour can be observed in the {position_description} area of the image.
2. Q: Specify the tumour’s location in this {image_type} view.
A: The tumour is clearly seen in the {position_description} region.
3. Q: Can you detect where the tumour is in this {image_type}?
A: The tumour is present in the {position_description} area.
4. Q: Which area of the {image_type} shows a tumour?
A: The tumour is seen in the {position_description}.
5. Q: From this {image_type} scan, what is the tumour’s location?
A: The tumour is located in the {position_description} part.
6. Q: Could you point out where the tumour is located in this {image_type} scan?
A: The tumour can be observed in the {position_description} area of the
{image_type} scan.
7. Q: What part of the body does the tumour appear in this {image_type} medical image?
A: In this {image_type} image, the tumour is found in the {position_description}
section.
8. Q: Identify the region in this {image_type} image that shows the tumour.
A:
The
region
showing
the
tumour
in
this
{image_type}
image
is
the
{position_description}.
9. Q: Can you specify the tumour’s location based on the {image_type} image provided?
A:
Based
on
the
provided
{image_type}
image,
the
tumour
lies
in
the
{position_description} region.
10. Q: From this {image_type} image, where would you say the tumour is located?
A:
Judging
from
the
{image_type}
image,
the
tumour
is
located
at
the
{position_description}.
17


## Page 18

11. Q: Could you specify the tumour’s location in this {image_type} scan?
A: The tumour is clearly located in the {position_description} area.
12. Q: What region of this {image_type} shows the presence of a tumour?
A: The tumour is seen in the {position_description} region.
13. Q: Where in the anatomical image ({image_type}) is the tumour located?
A: It is present in the {position_description} portion of the anatomy.
14. Q: Is there a visible tumour in the {image_type} image, and where?
A: Yes, it is found in the {position_description} area.
15. Q: In which region of the {image_type} can the tumour be found?
A: The tumour can be found in the {position_description}.
16. Q: Pinpoint the tumour location in this {image_type} scan.
A: It is located in the {position_description} region.
17. Q: Which part of the {image_type} image contains the tumour?
A: The tumour is contained in the {position_description} part.
18. Q: In this {image_type} image, what is the tumour’s anatomical position?
A: The anatomical position of the tumour is {position_description}.
19. Q: Identify the segment of this {image_type} that has a tumour.
A: The segment is {position_description}.
20. Q: Where is the abnormal mass located in this {image_type} scan?
A: The abnormal mass appears in the {position_description}.
21. Q: Can you detect the tumour’s placement in the {image_type} image?
A: The placement is in the {position_description} zone.
22. Q: Is the tumour visible in this {image_type}, and where is it found?
A: Yes, it is located in the {position_description} portion.
23. Q: Which anatomical zone in the {image_type} image shows a tumour?
A: It is visible in the {position_description} region.
24. Q: Where does the tumour appear in this {image_type} scan?
A: It appears in the {position_description} region of the scan.
25. Q: Indicate the region where the tumour is located in this {image_type}.
A: The region is the {position_description}.
26. Q: In this scan of {image_type}, where do you see the tumour?
A: The tumour is seen in the {position_description} area.
27. Q: What area in the {image_type} image reveals the tumour?
A: The area is {position_description}.
28. Q: According to this {image_type} image, where is the tumour found?
A: It is found in the {position_description}.
29. Q: What is the approximate tumour position in this {image_type}?
A: Approximately, it lies in the {position_description}.
30. Q: Give the precise tumour location in this {image_type} image.
A: It is precisely located in the {position_description}.
31. Q: Can the tumour be located in the upper or lower part of the {image_type}?
A: It is found in the {position_description} section.
32. Q: Which side of the {image_type} contains the tumour?
A: The tumour is on the {position_description} side.
33. Q: In this {image_type} scan, which quadrant has the tumour?
A: The {position_description} quadrant contains the tumour.
34. Q: What part of the {image_type} is affected by the tumour?
A: The {position_description} part is affected.
35. Q: Where is the main tumour mass observed in this {image_type}?
A: It is observed in the {position_description} region.
18


## Page 19

36. Q: Describe the tumour’s spatial location in this {image_type} scan.
A: The spatial location corresponds to the {position_description}.
37. Q: Where is the suspicious mass situated in this {image_type}?
A: It is situated at the {position_description}.
38. Q: Which image region shows the most tumour density in this {image_type}?
A: The region with most density is the {position_description}.
39. Q: Can you tell which section of the image highlights the tumour?
A: The highlighted tumour appears in the {position_description} section.
40. Q: In this {image_type} medical scan, where can the tumour be localized?
A: It can be localized in the {position_description} area.
41. Q: Where is the focal point of the tumour in this {image_type}?
A: The focal point is at the {position_description}.
42. Q: Which directional area of the {image_type} shows the tumour?
A: The tumour shows up in the {position_description} direction.
43. Q: Can you indicate the approximate region where the tumour lies?
A: It lies approximately in the {position_description}.
44. Q: Where would you mark the tumour in this {image_type}?
A: I would mark the tumour in the {position_description}.
45. Q: In this view of the {image_type}, what part contains the tumour?
A: The tumour is in the {position_description} view.
46. Q: What’s the visible tumour location in the {image_type} image?
A: Visibly, it is in the {position_description}.
47. Q: According to the image, where does the tumour appear?
A: It appears in the {position_description} area.
48. Q: From the given {image_type}, where can we see the tumour?
A: It is seen in the {position_description} region.
49. Q: What is the rough location of the tumour in the image?
A: Roughly, the tumour is at the {position_description}.
50. Q: Could you highlight the tumour’s location in this {image_type} image?
A: The tumour is highlighted in the {position_description} region.
For the test dataset, we use 5 following templates for testing the performance of the model:
1. Q: Can you identify the location of the tumour in this {image_type} medical image?
A: The tumour is located in the {position_description} region of the {image_type}
image.
2. Q: Please describe the tumour’s position in this medical image of types {image_type}.
A:
In
this
{image_type}
medical
image,
the
tumour
appears
in
the
{position_description}.
3. Q: What is the anatomical location of the tumour in this {image_type} medical image?
A:
The
anatomical
position
of
the
tumour
in
this
{image_type}
image
is
{position_description}.
4. Q: Based on this {image_type} medical image, can you provide the location of the tumour
in this image?
A: From the {image_type} image, the tumour is seen in the {position_description}
area.
5. Q: Where is the tumour located in this {image_type} medical image?
A: The tumour is situated in the {position_description} part of the {image_type}
image.
19


## Page 20

A.9
Demo visualization
In this section, we present visualizations of the questions, answers, and segmentation masks across
different data points covering all six specifications, as shown in Figure 7 and Figure 8. Each sample
includes a question posed by the doctor, the corresponding answer generated by the assistant, and the
resulting segmentation mask.
Figure 7: More visualization of the inference results for MRI, endoscopy, and ultrasound data.
20



### 图片

![2505.11872v3_page20_img1.jpeg](images/2505.11872v3_page20_img1.jpeg)

## Page 21

Figure 8: More visualization of the inference results for skin, Lung Xray, and Lung CT data.
A.10
Limitation of our methods
Reasoning limitations: Although the promising results of our method are improved, the hallu-
cination remains a challenge as same as previous Multimodal-LLM. An example of this issue is
illustrated as follows:
• Question: Can you identify the tumour location in the right lung?
• Answer: I cannot provide specific details about the location of the tumour in the image.
However, the image is a CT scan, which can help visualize the tumour and its relationship to
surrounding structures. To accurately identify the location of the tumour, it would be helpful
to have more information about the patient’s anatomy and the specific area being examined.
• Question: Can the tumour be identified in this endoscopy image?
21



### 图片

![2505.11872v3_page21_img1.jpeg](images/2505.11872v3_page21_img1.jpeg)

## Page 22

• Answer: I cannot identify the exact location of the tumour in the image. however, it is
important to note that the image is a ct medical image, which provides detailed cross-
sectional views of the body. to accurately identify the location of the tumour, a healthcare
professional would need to analyze the image in conjunction with the patient’s clinical
history and other diagnostic tests.
Segmentation limitations: Similar to the unified methods for medical image segmentation, our
method still has the challenge related to the boundary, which is the key to generating the highly
accurate segmented mask, while this can make the model generate redundant features and also can
not cover the complex boundary of the mask. The highlight for the failure is illustrated in Figure 9
Figure 9: Failure in the segmentation results of our methods.
In the further research, we encourage researchers to add the handcraft features such as the edge
information to improve this boundary problem, thus enhance the accurate in the segmentation module,
further is the screening system.
A.11
License
We acknowledge the Llava-Med (MICROSOFT RESEARCH LICENSE TERMS), Llama 3 ((Llama
3 Community License Agreement), and TinySAM (Apache License).
Regarding the dataset, we use the BUSI dataset, LungCT, LungXray, BrainMRI, Kvasir-SEG,
ClinicDB, CVC300, ColonDB, ETIS-Polyric, and all follow CC-0 license.
A.12
Possibility of Missuse
Due to the sensitivity of the medical dataset and the generative ability of the Multimodal-LLM, this
model can be used to generate fake patient report or used to treat unknown people in the medical
domain. For this reason, we restrict the illegal usage of this model, and all access has to be verified
by the organization.
22



### 图片

![2505.11872v3_page22_img1.png](images/2505.11872v3_page22_img1.png)
