## Page 1

FetalCLIP: A Visual-Language Foundation Model
for Fetal Ultrasound Image Analysis
Fadillah Maani1†, Numan Saeed1†, Tausifa Jan Saleem1,
Zaid Farooq1, Hussain Alasmawi2, Werner Diehl3,
Ameera Mohammad3, Gareth Waring3, Saudabi Valappil3,
Leanne Bricker3, Mohammad Yaqub1*
1Department of Computer Vision, Mohamed bin Zayed University of
Artificial Intelligence, Abu Dhabi, UAE.
2Department of Machine Learning, Mohamed bin Zayed University of
Artificial Intelligence, Abu Dhabi, UAE.
3Corniche Hospital, Abu Dhabi Health Services Company (SEHA), Abu
Dhabi, UAE.
*Corresponding author(s). E-mail(s): mohammad.yaqub@mbzuai.ac.ae;
†Contributed Equally.
Abstract
Foundation models are becoming increasingly effective in the medical domain,
offering pre-trained models on large datasets that can be readily adapted for
downstream tasks. Despite progress, fetal ultrasound images remain a challenging
domain for foundation models due to their inherent complexity, often requir-
ing substantial additional training and facing limitations due to the scarcity
of paired multimodal data. To overcome these challenges, here we introduce
FetalCLIP, a vision-language foundation model capable of generating univer-
sal representation of fetal ultrasound images. FetalCLIP was pre-trained using
a multimodal learning approach on a diverse dataset of 210,035 fetal ultrasound
images paired with text. This represents the largest paired dataset of its kind
used for foundation model development to date. This unique training approach
allows FetalCLIP to effectively learn the intricate anatomical features present in
fetal ultrasound images, resulting in robust representations that can be used for a
variety of downstream applications. In extensive benchmarking across a range of
key fetal ultrasound applications, including classification, gestational age estima-
tion, congenital heart defect (CHD) detection, and fetal structure segmentation,
FetalCLIP outperformed all baselines while demonstrating remarkable generaliz-
ability and strong performance even with limited labeled data. We plan to release
the FetalCLIP model publicly for the benefit of the broader scientific community.
Keywords: Fetal Ultrasound, Foundation Model, Visual-Language Model
1
arXiv:2502.14807v3  [eess.IV]  19 Oct 2025


## Page 2

1 Introduction
Prenatal care is historically known to be transformed by the integration of ultra-
sound technology. Widely recognized for its accessibility, safety, and cost-effectiveness,
ultrasound now plays a critical role in monitoring fetal development in real time and
facilitating the early detection of congenital abnormalities [1–4]. However, despite its
transformative role, fetal ultrasound image interpretation remains inherently subjec-
tive and heavily operator-dependent, often challenged by subtle visual cues, complex
fetal anatomy, and significant inter-observer variability [5, 6]. These challenges can
lead to inconsistencies in clinical assessments, particularly in resource-limited settings
where access to highly trained sonographers is restricted [7]. Studies have shown sig-
nificant inter-observer variability in fetal ultrasound measurements, with differences
of approximately ±4.9% for head circumference (HC), ±8.8% for abdominal circum-
ference (AC), and ±11.1% for femur length (FL) [8]. These variabilities highlight the
subjectivity and potential inconsistencies in fetal ultrasound assessments, emphasiz-
ing the need for AI-powered tools to enhance diagnostic objectivity and accuracy.
These variabilities underscore the subjectivity and potential inconsistencies in fetal
ultrasound assessments, highlighting the urgent need for robust AI-powered tools that
enhance diagnostic objectivity, accuracy, and accessibility in fetal ultrasound imaging.
Recent advances in artificial intelligence (AI), particularly in foundation mod-
els, have demonstrated remarkable capabilities in improving medical imaging analysis
[9, 10]. These models, pretrained on large-scale datasets, enable powerful fea-
ture extraction and knowledge transfer to downstream tasks, enhancing diagnostic
precision, optimizing clinical workflows, and broadening access to expert-level interpre-
tation. However, existing foundation models [11–26] exhibit fundamental limitations
when applied to fetal ultrasound. For instance, CLIP [11], primarily trained on nat-
ural images, lacks the domain-specific anatomical knowledge necessary for medical
imaging. BiomedCLIP [12], while tailored for the biomedical domain, is primarily opti-
mized for text-based biomedical knowledge retrieval rather than complex image-text
reasoning required for fetal ultrasound interpretation, limiting its ability to effectively
capture fine-grained anatomical details. UniMed-CLIP [13], though leveraging a large-
scale open-source medical multimodal dataset, demonstrates variable performance
across different imaging modalities and remains largely unexplored in the context of
fetal ultrasound. Complementary to the aforementioned approaches, numerous efforts
have been made to develop foundation models for specific medical imaging modali-
ties, including chest X-ray [14–16], ophthalmic imaging [17, 18], pathology [19–21],
echocardiography [22–24], chest CT [25], and head CT [26]. These models struggle to
generalize to the unique challenges posed by fetal imaging, failing to capture subtle
morphological variations that are crucial for assessing fetal development. Moreover,
most AI solutions in fetal ultrasound rely on limited datasets and do not achieve the
level of generalizability required for robust clinical deployment, particularly in detect-
ing rare fetal conditions. Encoding the entirety of fetal anatomy while preserving
diagnostically critical features remains a significant challenge for existing AI methods.
To overcome these barriers, a dedicated fetal ultrasound-specific foundation model is
required to harness the full potential of AI for advancing prenatal care.
Here, we introduce FetalCLIP, a novel visual-language foundation model explicitly
engineered for fetal ultrasound analysis, trained on the most comprehensive dataset
of its kind. FetalCLIP is pretrained at an unprecedented scale (Fig. 1a), leverag-
ing 207,943 fetal ultrasound images with corresponding GPT-4o-generated captions
and 2,092 expert-annotated image-caption pairs from a fetal ultrasound textbook.
This extensive dataset, which was curated by authors in collaboration with medi-
cal experts, covers a broadspectrum of fetal anatomical structures and developmental
stages, ensuring both diversity and robustness. FetalCLIP incorporates an innovative
multimodal contrastive learning strategy that integrates visual and textual represen-
tations of fetal ultrasound data, allowing the model to effectively align anatomical
2


## Page 3

structures with diagnostic descriptions and enhance interpretability. This advanced
pretraining paradigm empowers FetalCLIP to learn rich, generalizable representations
of fetal ultrasound scans and effectively transfer this knowledge to a diverse range
of downstream tasks, including zero-shot classification of standard fetal views, con-
genital heart disease (CHD) detection from ultrasound videos, segmentation of fetal
anatomical structures, and feature extraction for downstream fetal ultrasound tasks,
ensuring adaptability across clinical applications. Unlike existing models, FetalCLIP’s
dual-modality learning approach allows it to discern subtle, clinically actionable pat-
terns in fetal ultrasound images, surpassing the capabilities of vision-only models and
yielding substantial improvements in diagnostic accuracy and clinical interpretability.
By addressing the critical gaps in fetal ultrasound analysis, FetalCLIP represents
a significant step toward more reliable, accessible, and AI-driven prenatal diagnostics.
FetalCLIP undergoes rigorous evaluation across multiple downstream tasks to assess
its generalizability and clinical applicability. It achieves an 87.1% F1 score in zero-shot
classification of standard fetal views, outperforming existing foundation models and a
state-of-the-art open-source fetal view classifier trained with supervised learning [27].
In the critical task of congenital heart disease (CHD) detection from ultrasound videos,
FetalCLIP demonstrates 6.92% improvement in AUC over previous models, showcas-
ing its ability to detect subtle morphological variations crucial for early diagnosis.
Furthermore, for fetal anatomical segmentation, it attains an average Dice Similarity
Coefficient (DSC) of 84.22% across three different fetal anatomical planes, highlighting
its proficiency in delineating fetal structures with high precision. These findings firmly
establish FetalCLIP as a pivotal advancement, bridging the gap between human-level
expertise and AI-driven prenatal diagnostics and setting a new benchmark for the field
of fetal ultrasound analysis.
2 Results
2.1 Overview of FetalCLIP
FetalCLIP is a vision-language foundation model for fetal ultrasound analysis, devel-
oped by leveraging a large-scale dataset of paired fetal ultrasound images and captions
(Fig. 1a). The dataset comprises 207,943 images from routine clinical scans encompass-
ing 64 diverse clinician-labeled keywords across 6,493 patients with a mean gestational
age of 148 ± 16 days (21 weeks 1 day ± 2 weeks 2 days), supplemented by 2,092
image-caption pairs derived from a fetal ultrasound textbook [28] primarily focused on
the fetal heart. As expert-provided image-level text descriptions were absent for the
clinical dataset, we used GPT-4o to generate clinically sound captions based on ges-
tational age, clinical labels, and pixel spacing, with details provided in the Methods.
This approach ensured each image was paired with a unique, contextually relevant
textual description. We adopted the contrastive language-image pretraining (CLIP)
[11] framework to pretrain FetalCLIP by aligning images with their corresponding
textual descriptions in a shared embedding space while minimizing similarity to unre-
lated pairs (Fig. 1b). The FetalCLIP architecture includes a ViT-L [29] image encoder,
a Byte-Pair Encoding tokenizer [30], and a text encoder [31] capable of processing
up to 117 tokens to accommodate the maximum token length in the rich clinical
text descriptions. Our extensive evaluations demonstrated the superior performance
of FetalCLIP over existing vision-language foundation models across various tasks in
fetal ultrasound analysis (Fig. 1c), attributed to its pretraining on a large-scale dataset
comprising predominantly clinical data with diverse fetal ultrasound keywords (Fig.
1d).
3


## Page 4

2.2 Zero-shot classification of standard fetal views
We conducted a study to evaluate FetalCLIP’s capability in classifying standard
fetal ultrasound views using unseen data from different hospitals without any adap-
tation (Fig. 2a). FetalCLIP was compared against notable models in the field: (1)
SonoNet [27], a specialized model specifically trained for classifying standard views
of fetal ultrasound; (2) CLIP [11], a visual-language model (VLM) for natural
images; and two visual-language foundation models tailored for the general medical
domain, (3) BiomedCLIP [12] and (4) UniMed-CLIP [13]. We conducted the eval-
uation using the Planes DB [32] dataset, which consists of fetal ultrasound images
acquired from two hospitals. The models were employed to classify five anatomical
planes—abdomen, brain, cervix, femur, and thorax—as well as three subplanes within
the brain—transcerebellum, transthalamic, and transventricular. Our experiment
(Fig. 2b) demonstrated that FetalCLIP achieved an average F1 score of 87.1%, outper-
forming SonoNet by a notable margin of 17.2%, UniMed-CLIP by 37.6%, BiomedCLIP
by 40.5%, and CLIP by 60.1%. The findings also highlight that CLIP lacks the contex-
tual information required to differentiate between different fetal ultrasound anatomical
views, and both BiomedCLIP and UniMed-CLIP struggle in distinguishing between
brain subplanes. Our confusion matrix analysis (Extended Data Fig. 1) further added
that SonoNet struggled with identifying the cervix view, while UniMed-CLIP could
not distinguish the spine view from other fetal planes. In contrast, by harnessing
large-scale fetal ultrasound data and incorporating semantic knowledge from textual
descriptions during pretraining, FetalCLIP achieved remarkable accuracy in zero-shot
classification of standard anatomical views.
2.3 Zero-shot gestational age estimation
The FetalCLIP pretraining stage requires the model to incorporate gestational age in
order to precisely align fetal ultrasound images with their corresponding text descrip-
tions. This enables FetalCLIP to estimate gestational age (GA) directly from the
images (Fig. 2c) to a certain degree of precision, without additional fine-tuning, as
detailed in the Methods section. To evaluate its zero-shot performance for this spe-
cific task, we leveraged the HC18 [33] dataset, which includes fetal brain images along
with head circumference (HC) annotations and pixel spacing. As the true GA was not
available, we defined a prediction to be valid if the true HC falls within the 2.5th to
97.5th percentile range of HC for the predicted GA, which was calculated using the
quantile regression method established by the WHO [34]. FetalCLIP achieved a pre-
diction validity rate of 83.5%, while other models produced much lower validity rates
(CLIP: 11%, BiomedCLIP: 24%, and UniMed-CLIP: 9%). This evaluation (Fig. 2d)
also highlights that while existing visual-language foundation models failed to reliably
infer GA from images, FetalCLIP effectively produced a high proportion of valid GA
predictions. Our further investigation demonstrated that FetalCLIP achieved a higher
validity rate of 89% within the range of true HC values associated with the 25th to 75th
percentile of the pretraining GA distribution (20 weeks 0 days to 21 weeks 6 days).
This suggests that FetalCLIP’s pretraining data influenced its zero-shot performance
in estimating GA.
2.4 FetalCLIP as strong feature extractor for fetal US
Motivated by the growing need for efficient tuning to adapt large pre-trained mod-
els to diverse applications—an essential consideration in building large and complex
AI systems [35–38]—we assessed the capability of the FetalCLIP image encoder to
extract generalizable features for downstream fetal ultrasound tasks. In this setup, the
image encoder was entirely frozen, while a lightweight network was trained to utilize
the extracted features for a specific ultimate task—e.g. a linear layer for classification.
4


## Page 5

This setting heavily relies on the image encoder to provide robust image represen-
tations. We evaluated the image encoder on the following tasks: (1) standard view
classification in fetal ultrasound, (2) congenital heart defect (CHD) detection from
fetal ultrasound videos, and (3) segmentation of fetal anatomical structures across
different views. Our experiments (Fig. 1c) demonstrated that FetalCLIP consistently
outperformed other visual-language foundation models across all tasks, highlighting its
superior performance as a feature extractor capable of delivering generalizable image
representations for fetal ultrasound analysis.
2.4.1 Probing FetalCLIP for fetal views classification
In some clinical practices, adapting a foundation model is essential to achieve optimal
accuracy in identifying specific fetal views from datasets obtained from different hospi-
tals. To this end, we investigated the FetalCLIP ability to provide robust features for
accurately distinguishing six fetal ultrasound views and three brain subplanes using
the Planes DB dataset. We attached a single linear layer to the frozen FetalCLIP
image encoder, harnessing its image representations and transforming them into pre-
dictions for the set of output views (Fig. 3a). Across all views, FetalCLIP achieved
significantly higher F1 scores compared to CLIP, BiomedCLIP, and UniMed-CLIP
(Fig. 3b-c). In addition, we also evaluated their performance in a data-efficient setting,
using data from only a few patients for training. Similarly, FetalCLIP consistently
outperformed both the natural and medical vision foundation models (Extended Data
Fig. 3a-b). Using data from only 32 patients, FetalCLIP demonstrated comparable or
even superior accuracy to UniMed-CLIP trained on the full dataset of 717 patients.
2.4.2 Probing FetalCLIP for video-based CHD detection
Clinicians are often required to analyze ultrasound videos to assess fetal conditions,
such as detecting abnormalities in the fetal heart. However, developing AI solutions
for such tasks is challenging given the limited availability of annotated ultrasound
video data. Thus, leveraging pretrained models is critical for enhancing model general-
izability. Motivated by this, we adapted FetalCLIP to analyze fetal ultrasound videos
focusing on the 4-chamber view, with the aim of distinguishing between normal fetal
hearts and those affected by congenital heart disease (CHD) (Fig. 3d). We leveraged
FetalCLIP to extract image features from each frame of the ultrasound videos. These
frame-level features were then combined (see Methods), and a linear layer was applied
to classify the fetal heart as either normal or having CHD. Our experiment demon-
strated that FetalCLIP outperformed the other foundation models by a substantial
margin (Fig. 3e-f), achieving a mean AUROC of 78.72%, whereas CLIP, BiomedCLIP,
and UniMed-CLIP achieved 67.88% (P < 10−5), 64.32% (P < 10−6), and 71.8%
(P < 10−3) respectively. This showcases the excellent adaptability of FetalCLIP in
analyzing fetal ultrasound videos.
2.4.3 Probing FetalCLIP for segmenting fetal structures
Accurate pixel-level classification is critical for precise growing fetal biometry calcula-
tions [33, 39]. We investigated the foundation models’ ability to provide fine-grained
intermediate image features essential for localizing fetal anatomical structures. We
apply a lightweight decoder with few parameters (∼1.3 million parameters for ViT-B
and ∼1.6 million for ViT-L encoders) to utilize the intermediate image features for
accurate segmentation of fetal anatomical structures (Fig. 4a). We conducted segmen-
tation experiments on three fetal views to segment various structures: 1) brain view
(head), 2) abdomen view (abdomen, stomach, and spine), and 3) 4-chamber view (nine
structures, see Extended Data Table 1). We reported the average Dice Similarity Coef-
ficient (DSC) for each view and weighted each structure equally. FetalCLIP achieved
DSC of 97.92%, 81.82%, and 72.91% for brain, abdomen, and 4-chamber views,
5


## Page 6

respectively, surpassing UniMed-CLIP by 0.08% (P < 10−6), 1.7% (P < 10−3), and
3.64% (P < 10−7) (Fig. 4b-d). A similar trend was observed in data-efficient settings
(Extended Data Fig. 3c-e), where FetalCLIP consistently outperformed other founda-
tion models. These results demonstrated that FetalCLIP can serve as a robust feature
extractor for fetal ultrasound tasks requiring fine-grained anatomical segmentation.
2.5 FetalCLIP interpretability
To analyze the FetalCLIP reliability from the clinical perspective, we conducted inter-
pretation studies to investigate how the model derives its prediction and assess whether
it aligns with clinical practice. We employed class activation mapping (CAM) via
ScoreCAM [40] to visualize the importance of image regions to FetalCLIP’s decisions,
as depicted in Fig. 5a-b. The CAMs in Fig. 5a suggested that FetalCLIP can effectively
highlight key fetal landmarks when identifying anatomical views, such as stomach
in the abdomen view, femur bone, heart circumference, cerebellar hemispheres, and
cavum septum pellucidi. This showcased FetalCLIP’s ability to localize specific struc-
tures within fetal ultrasound images. Furthermore, as visualized in Fig. 5b, FetalCLIP
highlighted regions surrounding the head circumference and some brain structures,
such as the choroid plexus, to estimate gestational age.
To gain a deeper understanding of FetalCLIP’s image representations, we utilized
the Uniform Manifold Approximation and Projection (UMAP) [41] to visualize Fetal-
CLIP image embeddings in a two-dimensional space. We found that FetalCLIP could
effectively cluster five standard fetal planes (Fig. 5c) and differentiate between brain
subviews (Fig. 5d). Our further investigation (Fig. 5e) revealed that FetalCLIP could
cluster other fetal views such as profile and spine, as well as non-anatomical elements
like tables, and could map images containing related anatomical structures into close
proximity, such as the fetal extremities, which includes the leg, feet, arm, and hand.
These observations were then verified by clinicians to confirm their validity. Thus,
this finding showcases FetalCLIP’s ability to enable automatic plane classification and
clustering, potentially improving workflow efficiency in clinical practice.
3 Discussion
The advent of foundation models has shifted the field of image analysis toward an
exciting paradigm. However, unlike in the natural domain, developing foundation
models for the medical domain is more challenging due to the high heterogeneity of
medical modalities. While recent studies have shown that modality-specific medical
foundation models often outperform general medical foundation models [13, 16–20],
both approaches have largely ignored fetal ultrasound. This slow advancement in fetal
ultrasound can be attributed to the data scarcity in this domain that is both sufficient
in quantity and contextual information. This study introduces FetalCLIP as the first
visual-language foundation model designed explicitly for fetal ultrasound.
In this study, our findings suggest that despite the absence of large image-clinical
text description pairs, a strong visual-language foundation model can be developed by
leveraging routine pregnancy scans with limited contextual information, supplemented
by image-caption pairs from a textbook. Unlike other foundation models that place
limited emphasis on fetal ultrasound, FetalCLIP is highly adaptable across various
fetal ultrasound tasks. Despite not being explicitly trained for specific tasks, Fetal-
CLIP achieved excellent zero-shot classification of different fetal anatomical planes,
surpassing a specialist model for view classification (SonoNet [27]), even when tested
on unseen data from multiple hospitals. Pretrained with over 64 clinician-labeled
keywords, FetalCLIP could reduce the labor-intensive process of manually identify-
ing fetal ultrasound images, thus improving the accuracy and efficiency of prenatal
assessment, especially in rural or low-resource environments. Furthermore, FetalCLIP
is equipped with zero-shot capability to estimate gestational age from fetal images
6


## Page 7

with remarkable accuracy. This highlights the presence of the FetalCLIP knowledge to
extract meaningful information from fetal structures for assessing fetal growth. How-
ever, FetalCLIP struggles to accurately estimate gestational age for fetuses in early
and late gestation. We hypothesize that this limitation arises from the distribution
of its pretraining data, with the majority of the images acquired during the second
trimester. Expanding pretraining data to cover a broader gestational age range could
further improve performance.
This study also demonstrates that FetalCLIP can serve as a robust feature extrac-
tor. Our extensive downstream experiments across diverse fetal ultrasound tasks
revealed substantial performance gains of FetalCLIP over other foundation models.
The experiments included standard fetal plane classification, brain subviews classifi-
cation, CHD detection from four-chamber videos, and segmentation of different fetal
structures in the head, abdomen, and four-chamber views. These results establish our
visual-language foundation model as the most preferred pretrained model for devel-
oping AI models to solve challenging problems in fetal ultrasound analysis, especially
in data-efficient settings. In addition, these findings align with recent studies report-
ing the superiority of modality-specific foundation models over their general medical
counterparts when applied to their respective targeted modalities [13, 16–20].
In addition to FetalCLIP’s superior performance compared to other visual-language
foundation models, our interpretability analysis underscores FetalCLIP’s reliability
and alignment with clinical practice. Using ScoreCAM, we demonstrated that Fetal-
CLIP effectively localizes relevant anatomical structures and regions of interest when
identifying anatomical views (Fig. 5a), such as the stomach in the abdomen view and
cerebellar hemispheres in the transcerebellar plane. This CAM analysis also demon-
strates that, when estimating gestational age, the model focuses on regions such as
the head circumference and brain structures, including the choroid plexus (Fig. 5b).
Moreover, UMAP visualizations (Fig. 5c-e) revealed that FetalCLIP’s embeddings can
effectively cluster standard fetal planes and differentiate between brain subviews, while
mapping related anatomical structures into close proximity, e.g. fetal extremities.
FetalCLIP’s significant gains in zero-shot, transfer learning, and interpretability
contribute to advancing fetal ultrasound image analysis, with future improvements
possible through enriched pretraining data. While our zero-shot results demonstrate
substantial performance gains of FetalCLIP over existing visual-language foundation
models, we anticipate that the FetalCLIP’s zero-shot capability for detecting abnor-
malities in fetal ultrasound scans may still be limited. This limitation likely arises
from the fact that the majority of FetalCLIP pretraining data was collected from
routine pregnancy scans in the second trimester which lacked information on fetal
health conditions. Nevertheless, FetalCLIP exhibits superior performance and higher
adaptability across various tasks compared to existing foundation models, which were
typically trained on millions of image-caption pairs. This leaves immense potential for
future studies to enhance FetalCLIP’s representations by incorporating more diverse
fetal ultrasound images and richer image-level descriptions. Additionally, due to com-
putational constraints and to facilitate a fair benchmark with compared foundation
models, we restricted our experiments to 224 × 224 image size. Notably, leveraging
higher resolutions could elevate FetalCLIP’s capabilities by providing clear fine-grained
details such as valves in fetal hearts and brain structures. This study also underscores
potential future exploration that includes expanding the pretraining data to cover
wider data distribution, more diverse clinical scenarios, and a broader range of image
types, as well as extending FetalCLIP to a video encoder for better capturing spatio-
temporal features, thus broadening its applicability and impact in fetal ultrasound
analysis. By releasing FetalCLIP to the public, we aim to foster the advancement of
fetal ultrasound analysis by enabling researchers and clinicians to develop innovative
applications on top of this strong foundation model.
7


## Page 8

4 Methods
4.1 FetalCLIP pretraining data curation
FetalCLIP was pretrained using two data sources, curated to develop a robust and
generalizable visual-language foundation model for fetal ultrasound image analysis.
The first data source consists of 207,943 fetal ultrasound images from Corniche Hos-
pital Abu Dhabi, a tertiary-level referral unit providing expert care to women and
neonates. This data was supplemented by 2,092 image-caption pairs from a fetal ultra-
sound textbook [28] emphasizing fetal heart conditions, providing critically important
clinical information for fetal health assessment in clinical practice.
4.1.1 Private hospital dataset
This data source constitutes the largest portion of the FetalCLIP pretraining dataset.
The images were obtained from routine pregnancy scans at the hospital to monitor
fetal development, often performed during the second trimester. During this stage, the
fetus begins to resemble a baby, and sonographers examine various anatomical planes
to detect any potential health abnormalities. The average gestational age in this data
source is 148 ± 16 days, with 50% of the images lying between 20 weeks 0 days and 21
weeks 6 days. Some images were accompanied by clinicians’ text annotations embedded
within the images, from which we identified 64 distinct keywords predominantly related
to fetal anatomical structures (Fig. 1d). However, no associated medical report is
available for each image. For this study, we utilized B-mode images and leveraged
text annotations written by clinicians to serve as the basis for image labeling and text
description generation for each image.
A considerable effort was made to preprocess, clean, and standardize this data
source. We began by detecting clinicians’ text annotations using the EasyOCR [42]
library, followed by manual text processing to clean and standardize the detected
annotations, resulting in the 64 keywords. In addition, to prevent potential model
shortcuts, such as reliance on text annotations within the images, we designed a robust
image preprocessing pipeline. The ultrasound fan region was extracted by first isolating
the foreground, retaining all non-zero pixel values. We then applied the findCon-
tours function from the OpenCV [43] library and identified the largest contour as the
ultrasound fan region. To address text annotations, colored regions within the images
were detected and inpainted using the Fast Marching Method [44]. These preprocess-
ing steps removed confounding information from fetal ultrasound images, facilitating
unbiased model training.
Based on the clinician annotations, we categorized this data source into three
subgroups: (1) common standard anatomical views, (2) images with diverse clinical
keywords, and (3) images without text annotations.
Subgroup 1. This subgroup comprised 88,045 images labeled across 12 standard
anatomical views [45], including the abdomen, brain, cord, diaphragm, feet, femur,
heart, kidney, lips & nose, orbit, profile, and spine, with some examples shown in
Extended Data Fig. 2a. To ensure high-quality labeling, a confident learning algo-
rithm [46] was employed to detect potentially mislabeled samples. This process
involved training a 5-fold cross-validation (CV) model to classify the 12 views, using
a patient-wise dataset split to avoid data leakage across folds. The algorithm identi-
fied mislabeled samples by comparing the model’s confidence in its predictions with
the provided labels. A total of 984 samples were identified as potentially mislabeled
(examples shown in Extended Data Fig. 2b) and subsequently excluded, resulting in
a final dataset of 87,061 labeled images.
For the heart images, clinicians provided subview annotations for LVOT, RVOT, 4-
CH, and 3VV/3VT, providing fine-grained information. In contrast, 468 brain images
(out of 5,297) were labeled by an expert into three brain subviews, i.e. transthalamic,
transcerebellum, and transventricular. These expert-labeled brain images were then
8


## Page 9

used to train a 5-fold CV brain subview classifier. Pseudo-labeling was applied to the
remaining brain images, assigning subviews to images with probabilities exceeding
90%. Images with lower probabilities were categorized under the general ”Brain” class.
Subgroup 2. This sub-data contains 73,972 images with diverse clinical labels, includ-
ing cases where multiple anatomical structures were visible in a single frame. As
illustrated in Extended Data Fig. 2c, these images showcase the complexity of captur-
ing multiple information within a single ultrasound frame, providing valuable examples
for foundation model pretraining.
Subgroup 3. This subgroup consists of 79,757 unlabeled images. A 5-fold CV 12-
view classifier trained on the first subgroup data was used to generate pseudo labels.
Only images with classification probabilities exceeding 90% were retained, resulting
in 46,910 pseudo-labeled images. Furthermore, labeled heart subviews and brain sub-
views were used to train a 5-fold CV classifier for providing fine-grained information
for pseudo-labels. Similarly, we assigned brain and heart images to their respective
subviews if the model confidence exceeded 90%.
Caption generation. We generated captions for fetal ultrasound images by inte-
grating information from the clinician text annotations or pseudo-labels, along with
gestational age and pixel spacing. Instead of generating captions for each individual
image, we efficiently constructed five caption templates for each unique set of clini-
cian text annotations using GPT-4o, as illustrated in Extended Data Fig. 4a-b. These
multiple caption templates also acted as a form of text augmentation that enabled
the model to learn more robust visual-language representations. The variation in ges-
tational age and pixel spacing across images ensured that the five captions for each
image were distinguishable from those of other ultrasound images.
4.1.2 Fetal ultrasound textbook dataset
We extracted 849 image-caption pairs from a textbook [28] focused on fetal heart con-
ditions. Since most figures in the textbook contained multiple subfigures, we manually
separated these subfigures into 2,092 independent images. The corresponding captions
were then divided into subcaptions and refined using GPT-4o to ensure they were self-
contained and to eliminate references to visual markers (e.g. arrows), as illustrated
in Extended Data Fig. 4c. Given that this data source was approximately 100 times
smaller than the hospital data source, we upsampled the image-caption pairs from this
source by a factor of 10 to increase its significance during FetalCLIP pretraining. In
addition, we employed data sharding [47] where each shard contained unique image-
caption pairs without duplicates. This strategy ensured that an image-caption pair
from this data source did not appear multiple times in a single batch, which could
otherwise negatively affect the contrastive learning process.
4.2 FetalCLIP architecture and pretraining
We developed the FetalCLIP model by adapting the architecture and training method-
ology employed in CLIP [11]. Additionally, our FetalCLIP pretraining code was built
on top of the OpenCLIP [47, 48] repository with some modifications to suit our objec-
tives. The FetalCLIP image encoder utilizes a ViT-L [29] architecture, featuring an
image input size of 224×224, a patch size of 14×14, and 24 transformer layers. For
the text encoder, we implemented a text transformer [31] with 12 transformer lay-
ers and a maximum input token of 117—40 tokens more than the original CLIP
model—to accommodate clinical text descriptions, which are typically more detailed
than natural captions. Both the image and text encoders project their respective
inputs into a shared 768-dimensional space. We pretrained the FetalCLIP model to
maximize the similarity between embeddings of paired fetal ultrasound image-caption
data while minimizing the similarity of unpaired images and captions. This pretrain-
ing strategy enables FetalCLIP to derive semantically rich feature embeddings from
fetal ultrasound images and their associated captions.
9


## Page 10

We applied data augmentation techniques during FetalCLIP pretraining, includ-
ing random rotation (θrotation ∈[−7◦, 7◦]), translation (θtranslation ∈[−0.05, 0.05]),
and color jittering (θbrightness, θcontrast, θsaturation ∈[0.85, 1.15]). We pretrained Fetal-
CLIP for 20 epochs using a learning rate of 5e-6, a warmup phase of 2,000 steps, a
cosine scheduler, and a weight decay of 0.1. Mixed-precision training was implemented
on 4×RTX A6000 GPUs, allowing a batch size of 140 per GPU. Model checkpoints
were saved after each epoch and evaluated on zero-shot standard view classification (as
in Section 2.2 and Section 4.3), retaining the checkpoint with the highest average F1
score. We explored various model initializations and found that fine-tuning the origi-
nal CLIP in the general medical domain provided a strong initialization (accessed from
[49]) for our FetalCLIP model, improving the zero-shot view classification performance
from 85.2% to 87.1%. This underscores the importance of aligning a foundation model
for natural domain to the medical domain as a crucial step for better initialization,
resulting in improved performance on more specific modalities.
4.3 Zero-shot view classification
For zero-shot standard view classification with vision-language foundation models
(VLMs), we first defined target classes and provided five text prompts per class to
enable prompt ensembling for improved robustness, as VLMs are sensitive to text
prompts. We used GPT-4o to generate the text prompts for inferencing as detailed
in Extended Data Fig. 5. Our investigation revealed that specifically for FetalCLIP,
incorporating caption templates of our routine prenatal scan data to generate text
prompts for inference improved the average F1 score by 2.74% across five standard
fetal planes: abdomen, brain, femur, heart, and cervix. During testing, each text prompt
was converted into a 768-dimensional text embedding using the VLM tokenizer and
encoder, and embeddings were averaged across prompts for every class. Input images
were similarly encoded into 768-dimensional embeddings, and classification was per-
formed by selecting the class with the highest cosine similarity between the image
embedding and the text embeddings. The average F1 score across all target classes
was reported as the main metric for performance.
This zero-shot performance was evaluated using the Planes DB [32] dataset. The
dataset contains six classes: abdomen, femur, brain, thorax, cervix, and a class labeled
”other,” representing diverse additional views. The brain category is further divided
into three fine-grained subviews: transcerebellum, transthalamic, and transventricular.
Each image was preprocessed by symmetrically padding with zeros to form a square
and subsequently resizing to a uniform dimension of 224×224. We first evaluated
the performance on distinguishing five standard views, excluding the ”other” class,
resulting in 8187 test images. To ensure a fair comparison with the specialist model
(SonoNet), nine standard views were selected as target classes, i.e. abdomen, brain,
femur, heart, kidney, lips & nose, profile, spine, and cervix. Secondly, we evaluated
the model’s ability to classify the three brain subviews, resulting in 2949 test images.
As SonoNet lacks dedicated classes for both cervix and transthalamic, we mapped the
”other” class in SonoNet to cervix and transthalamic for the first and second evalua-
tions, respectively. In addition, the performance of the SonoNet models varied across
sizes, with SN16, SN32, and SN64 achieving F1 scores of 67.5%, 69.9%, and 67.2%,
respectively, where the mid-sized model (SN32) demonstrated the best performance.
4.4 Zero-shot gestational age estimation
We utilized the HC18 dataset [33] to evaluate VLM’s ability to estimate gestational
age (GA). The dataset includes fetal brain images alongside head circumference (HC)
measurements and pixel spacing information. Since the true GA values are not pro-
vided, our evaluation was based on the ground-truth HC. The relationship between HC
and GA can be explained using the following quantile regression formula established
10


## Page 11

by the WHO [34]:
HC = b0 + b1t + b2t2 + b3t3 + b4t4
(1)
Here, t represents GA in days, and b0, b1, b2, b3, b4 ∈R are coefficients that depend
on the quantile of interest. Equation 1 applies to GA ranging from 14 to 40 weeks.
Consequently, we restricted the analysis to images with HC values between 100 mm
and 342 mm, corresponding to the 50th percentile HC at 14 and 40 weeks of GA,
respectively, resulting in a test set of 814 brain images. To assess the plausibility of the
predicted GA, we formulated a proxy validation task: predictions were deemed valid
if the true HC fell within the 2.5th to 97.5th percentile range (95% of the population
distribution) associated with the predicted GA.
Inspired by [22], we estimated GA by first extracting image embeddings and sub-
sequently constructing text prompts that describe the brain view, pixel spacing, and
GA values ranging from 14 weeks 0 days to 40 weeks. Similar to our zero-shot view
classifier, we provided five text prompts for each GA to enable ensembling. The cosine
similarity between the image and the text prompts was computed for each GA, and
ensembling was performed by averaging the cosine similarities across the five prompts
for a given GA. As a postprocessing step, the final GA prediction was selected as the
median of the GAs corresponding to the top 15 text prompts with the highest cosine
similarity to the image embeddings. This postprocessing step enhanced the accuracy
of GA estimation, raising the validity rate by +3.08% compared to selecting the GA
based solely on the highest cosine similarity.
4.5 Probing FetalCLIP for downstream tasks
4.5.1 Experimental setup
The probing experiments were conducted on downstream fetal ultrasound tasks to
evaluate the quality of image embeddings from FetalCLIP compared to other foun-
dation models. In these experiments, the image encoder was frozen, and a trainable
prediction head was attached. We compared FetalCLIP with both natural and med-
ical vision-language foundation models across various fetal ultrasound benchmarks,
with P values computed using the Wilcoxon signed-rank test. Unless otherwise speci-
fied, for each evaluation, the corresponding dataset was split by patients into training
(80%) and testing (20%) sets to prevent information leakage in the test data due to
patient attributes, with stratified splitting employed for classification tasks. The pre-
diction head was tuned and validated using the training set, leaving the testing set
remain untouched during model development. In addition, we applied the same pre-
processing steps as in the zero-shot experiments, resulting in standardized images with
dimensions of 224×224. We then augmented the training images offline multiple times
and stored them locally, enabling pre-computation of image embeddings to accelerate
the training process. Details of the dataset used for downstream tasks along with the
training configurations are presented in Extended Data Table 1.
We considered two training scenarios: full-data training and data-efficient training
with few patients. For full-data training experiments, we performed stratified 5-fold
cross-validation on the training set. The prediction head was trained, and the model
with the lowest validation loss was evaluated on the test set. To achieve reliable sta-
tistical evaluation, each fold was run five times with different random seeds, resulting
in a total of 25 runs. For data-efficient training, we randomly selected five support
sets, each consisting of N patients for training and N for validation, while evaluation
was performed on the testing set. Each support set was run with five different random
seeds, resulting in a total of 25 outcomes per experiment.
4.5.2 View classification
The probing for view classification experiments was carried out using the Planes
DB [32] dataset, with the average F1 score computed as the performance metric. The
11


## Page 12

dataset comprises six categories of fetal ultrasound views with three brain subviews.
To harness VLM image embeddings, we attached a single trainable linear layer to the
output of the VLM image encoder, projecting the embeddings for the view prediction
task, which yields six output classes for view classification and three output classes
for fine-grained brain subview classification. We adhered to the original train-test split
provided in the dataset [32], resulting in 7129-5271 samples and 1543-1406 samples
for the first and second view classification tasks, respectively.
4.5.3 CHD detection
We evaluated the transferability of image embeddings from foundation models to fetal
ultrasound video analysis using an internal dataset. We collected 418 four-chamber
fetal heart ultrasound videos, comprising 161 normal and 257 abnormal scans, with
temporal lengths ranging from 16 to 128 frames, labeled by an expert in fetal cardiology
to ensure diagnostic accuracy. To simplify the task, instead of processing the entire
sequences, we extracted 16-frame clips for diagnosis. For videos with up to 64 frames,
we sampled clips with approximately uniform spacing, while for longer videos, clips
were sampled with a temporal stride of 4. This approach ensured that each clip spanned
at least 50% of the original video length, thus retaining adequate information to enable
diagnosis. Frame-level image features were extracted and then combined to generate
clip-level representations. We investigated two trivial feature combination methods,
with averaging and concatenating achieving AUROCs of 74.66 (P < 10−2) and 78.72,
respectively. To classify the heart as normal or abnormal, we implemented a single
linear layer that operated on the clip-level representations. We reported the area under
the receiver operating characteristic curve (AUROC) as the performance metric.
4.5.4 Segmentation
A lightweight decoder was developed to harness intermediate image representations
from foundation models for segmenting fetal anatomical structures. Specifically, we
adapted the UNETR [50] decoder for 2D applications while reducing its computa-
tional complexity and number of parameters. The deconvolution layers were replaced
with depthwise deconvolution layers, and convolutional blocks were replaced with
depthwise separable convolution [51] blocks with a kernel size of 3. This design modi-
fication resulted in a lightweight decoder with 1.32 M and 1.59 M parameters for the
ViT-B-based and ViT-L-based encoders, respectively. We reported the Dice Similarity
Coefficient (DSC) to assess the quality of the segmentation results. For views contain-
ing multiple segmented structures, we employed a multilabel segmentation approach,
wherein a single pixel could belong to multiple structures (e.g., a pixel might simulta-
neously represent the four-chamber view, heart, and thorax). This approach required
Ns output channels for Ns structures, with the average DSC across all structures
computed to give equal importance to each structure. Details of the data and hyperpa-
rameters for segmentation in brain, four-chamber, and abdominal views are provided
in Extended Data Table 1.
5 Data Availability
This study involved two publicly available datasets for evaluating models on a subset
of tasks, as well as private datasets for pretraining and additional evaluations. The
public datasets were obtained from PlanesDB (https://zenodo.org/records/3904280)
and HC18 (https://zenodo.org/records/1327317). Due to regulations and privacy
constraints, the private datasets cannot be released to the public.
12


## Page 13

6 Code Availability
The FetalCLIP code, including pretrained weights and prompts for inference, can be
accessed at https://github.com/BioMedIA-MBZUAI/FetalCLIP.
7 Acknowledgements
We gratefully acknowledge that this work was supported by GE Healthcare. We
express our gratitude to Corniche Hospital in Abu Dhabi for providing prenatal scan
data along with fetal heart scans, and to the Department of Health (DOH) Abu Dhabi
for their support in approving the study which facilitates access to the anonymous
data for internal purposes. We thank Alfred Z. Abuhamad for allowing us to leverage
his book for foundation model pretraining. We also thank GE Healthcare for provid-
ing data and annotations used for downstream segmentation tasks in the 4-chamber
and abdomen views.
13


## Page 14

References
[1] Whitworth, M., Bricker, L., Mullan, C.: Ultrasound for fetal assessment in early
pregnancy. Cochrane Database of Systematic Reviews (7) (2015) https://doi.org/
10.1002/14651858.CD007058.pub3
[2] Salomon, L.J., Alfirevic, Z., Bilardo, C.M., Chalouhi, G.E., Ghi, T., Kagan, K.O.,
Lau, T.K., Papageorghiou, A.T., Raine-Fenning, N.J., Stirnemann, J., Suresh,
S., Tabor, A., Timor-Tritsch, I.E., Toi, A., Yeo, G.: ISUOG practice guidelines:
performance of first-trimester fetal ultrasound scan. Ultrasound in Obstetrics &
Gynecology 41(1), 102–113 (2013) https://doi.org/10.1002/uog.12342
[3] Salomon, L.J., Alfirevic, Z., Berghella, V., Bilardo, C., Hernandez-Andrade, E.,
Johnsen, S.L., Kalache, K., Leung, K.Y., Malinger, G., Munoz, H., Prefumo,
F., Toi, A., Lee, W., Committee, I.C.S.: Practice guidelines for performance
of the routine mid-trimester fetal ultrasound scan. Ultrasound in Obstetrics &
Gynecology 37(1), 116–126 (2011) https://doi.org/10.1002/uog.8831
[4] Khalil, A., Sotiriadis, A., D’Antonio, F., Da Silva Costa, F., Odibo, A., Prefumo,
F., Papageorghiou, A.T., Salomon, L.J.: ISUOG Practice Guidelines: perfor-
mance of third-trimester obstetric ultrasound scan. Ultrasound in Obstetrics &
Gynecology 63(1), 131–147 (2024) https://doi.org/10.1002/uog.27538
[5] Chan, L.W., Fung, T.Y., Leung, T.Y., Sahota, D.S., Lau, T.K.: Volumetric (3d)
imaging reduces inter- and intraobserver variation of fetal biometry measure-
ments. Ultrasound in Obstetrics & Gynecology 33(4), 447–452 (2009) https:
//doi.org/10.1002/uog.6321
[6] Maraci, M.A., Napolitano, R., Papageorghiou, A., Noble, J.A.: Searching for
structures of interest in an ultrasound video sequence. In: Wu, G., Zhang, D.,
Zhou, L. (eds.) Machine Learning in Medical Imaging, pp. 133–140. Springer,
Cham (2014)
[7] Wanyonyi, S.Z., Mariara, C.M., Vinayak, S., Stones, W.: Opportunities and
challenges in realizing universal access to obstetric ultrasound in sub-saharan
africa. Ultrasound International Open 3(2), 52–59 (2017) https://doi.org/10.
1055/s-0043-103948
[8] Sarris, I., Ioannou, C., Chamberlain, P., Ohuma, E., Roseman, F., Hoch, L.,
Altman, D.G., Papageorghiou, A.T., Fetal, I., Century (INTERGROWTH-21st),
N.G.C.: Intra- and interobserver variability in fetal ultrasound measurements.
Ultrasound in Obstetrics & Gynecology 39(3), 266–273 (2012) https://doi.org/
10.1002/uog.10082
[9] Azad, B., Azad, R., Eskandari, S., Bozorgpour, A., Kazerouni, A., Rekik, I.,
Merhof, D.: Foundational Models in Medical Imaging: A Comprehensive Survey
and Future Vision (2023). https://arxiv.org/abs/2310.18689
[10] Awais, M., Naseer, M., Khan, S., Anwer, R.M., Cholakkal, H., Shah, M., Yang,
M.-H., Khan, F.S.: Foundation models defining a new era in vision: a survey and
outlook. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1–20
(2025) https://doi.org/10.1109/TPAMI.2024.3506283
[11] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G.,
Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable
visual models from natural language supervision. In: International Conference on
14


## Page 15

Machine Learning (2021). https://api.semanticscholar.org/CorpusID:231591445
[12] Zhang, S., Xu, Y., Usuyama, N., Xu, H., Bagga, J., Tinn, R., Preston, S., Rao,
R., Wei, M., Valluri, N., Wong, C., Tupini, A., Wang, Y., Mazzola, M., Shukla,
S., Liden, L., Gao, J., Lungren, M.P., Naumann, T., Wang, S., Poon, H.: Biomed-
CLIP: a multimodal biomedical foundation model pretrained from fifteen million
scientific image-text pairs (2024). https://arxiv.org/abs/2303.00915
[13] Khattak, M.U., Kunhimon, S., Naseer, M., Khan, S., Khan, F.S.: UniMed-CLIP:
Towards a Unified Image-Text Pretraining Paradigm for Diverse Medical Imaging
Modalities (2024). https://arxiv.org/abs/2412.10372
[14] Wang, Z., Wu, Z., Agarwal, D., Sun, J.: MedCLIP: Contrastive learning from
unpaired medical images and text. In: Goldberg, Y., Kozareva, Z., Zhang, Y.
(eds.) Proceedings of the 2022 Conference on Empirical Methods in Natural
Language Processing, pp. 3876–3887. Association for Computational Linguistics,
Abu Dhabi, United Arab Emirates (2022). https://doi.org/10.18653/v1/2022.
emnlp-main.256 . https://aclanthology.org/2022.emnlp-main.256/
[15] Xu, S., Yang, L., Kelly, C., Sieniek, M., Kohlberger, T., Ma, M., Weng, W.-H.,
Kiraly, A., Kazemzadeh, S., Melamed, Z., Park, J., Strachan, P., Liu, Y., Lau, C.,
Singh, P., Chen, C., Etemadi, M., Kalidindi, S.R., Matias, Y., Chou, K., Corrado,
G.S., Shetty, S., Tse, D., Prabhakara, S., Golden, D., Pilgrim, R., Eswaran, K.,
Sellergren, A.: ELIXR: Towards a general purpose X-ray artificial intelligence
system through alignment of large language models and radiology vision encoders
(2023). https://arxiv.org/abs/2308.01317
[16] Liang, X., Li, X., Li, F., Jiang, J., Dong, Q., Wang, W., Wang, K., Dong, S.,
Luo, G., Li, S.: Medfilip: Medical fine-grained language-image pre-training. IEEE
Journal of Biomedical and Health Informatics (2025)
[17] Silva-Rodr´ıguez, J., Chakor, H., Kobbi, R., Dolz, J., Ben Ayed, I.: A foundation
language-image model of the retina (flair): encoding expert knowledge in text
supervision. Medical Image Analysis 99, 103357 (2025) https://doi.org/10.1016/
j.media.2024.103357
[18] Shi, D., Zhang, W., Yang, J., Huang, S., Chen, X., Yusufu, M., Jin, K., Lin,
S., Liu, S., Zhang, Q., He, M.: EyeCLIP: A visual-language foundation model
for multi-modal ophthalmic image analysis (2024). https://arxiv.org/abs/2409.
06644
[19] Huang, Z., Bianchi, F., Yuksekgonul, M., Montine, T.J., Zou, J.: A visual–
language
foundation
model
for
pathology
image
analysis
using
medical
twitter. Nature Medicine 29(9), 2307–2316 (2023) https://doi.org/10.1038/
s41591-023-02504-3
[20] Lu, M.Y., Chen, B., Williamson, D.F.K., Chen, R.J., Liang, I., Ding, T., Jaume,
G., Odintsov, I., Le, L.P., Gerber, G., Parwani, A.V., Zhang, A., Mahmood, F.: A
visual-language foundation model for computational pathology. Nature Medicine
30(3), 863–874 (2024) https://doi.org/10.1038/s41591-024-02856-4
[21] Chen, R.J., Ding, T., Lu, M.Y., Williamson, D.F.K., Jaume, G., Song, A.H.,
Chen, B., Zhang, A., Shao, D., Shaban, M., Williams, M., Oldenburg, L.,
Weishaupt, L.L., Wang, J.J., Vaidya, A., Le, L.P., Gerber, G., Sahai, S., Williams,
W., Mahmood, F.: Towards a general-purpose foundation model for compu-
tational pathology. Nature Medicine 30(3), 850–862 (2024) https://doi.org/10.
15


## Page 16

1038/s41591-024-02857-3
[22] Christensen, M., Vukadinovic, M., Yuan, N., Ouyang, D.: Vision–language
foundation model for echocardiogram interpretation. Nature Medicine 30(5),
1481–1488 (2024) https://doi.org/10.1038/s41591-024-02959-y
[23] Vukadinovic, M., Tang, X., Yuan, N., Cheng, P., Li, D., Cheng, S., He, B., Ouyang,
D.: EchoPrime: A Multi-Video View-Informed Vision-Language Model for Com-
prehensive Echocardiography Interpretation (2024). https://arxiv.org/abs/2410.
09704
[24] Amadou, A.A., Zhang, Y., Piat, S., Klein, P., Schmuecking, I., Passerini,
T., Sharma, P.: EchoApex: A General-Purpose Vision Foundation Model for
Echocardiography (2024). https://arxiv.org/abs/2410.11092
[25] Hamamci, I.E., Er, S., Almas, F., Simsek, A.G., Esirgun, S.N., Dogan, I.,
Dasdelen, M.F., Durugol, O.F., Wittmann, B., Amiranashvili, T., Simsar, E.,
Simsar, M., Erdemir, E.B., Alanbay, A., Sekuboyina, A., Lafci, B., Bluethgen,
C., Ozdemir, M.K., Menze, B.: Developing Generalist Foundation Models from
a Multimodal Dataset for 3D Computed Tomography (2024). https://arxiv.org/
abs/2403.17834
[26] Zhu, W., Huang, H., Tang, H., Musthyala, R., Yu, B., Chen, L., Vega, E.,
O’Donnell, T., Dehkharghani, S., Frontera, J.A., Masurkar, A.V., Melmed, K.,
Razavian, N.: 3D Foundation AI Model for Generalizable Disease Detection in
Head Computed Tomography (2025). https://arxiv.org/abs/2502.02779
[27] Baumgartner, C.F., Kamnitsas, K., Matthew, J., Fletcher, T.P., Smith, S., Koch,
L.M., Kainz, B., Rueckert, D.: Sononet: Real-time detection and localisation of
fetal standard scan planes in freehand ultrasound. IEEE Transactions on Medical
Imaging 36(11), 2204–2215 (2017) https://doi.org/10.1109/TMI.2017.2712367
[28] Abuhamad, A.Z., Chaoui, R.: A Practical Guide to Fetal Echocardiography
Normal and Abnormal Hearts. Wolters Kluwer (2022)
[29] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby,
N.: An image is worth 16x16 words: Transformers for image recognition at scale.
ICLR (2021)
[30] Sennrich, R., Haddow, B., Birch, A.: Neural machine translation of rare
words with subword units. In: Erk, K., Smith, N.A. (eds.) Proceedings of
the 54th Annual Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pp. 1715–1725. Association for Computational
Linguistics, Berlin, Germany (2016). https://doi.org/10.18653/v1/P16-1162 .
https://aclanthology.org/P16-1162
[31] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I.: Language
models are unsupervised multitask learners (2019)
[32] Burgos-Artizzu, X.P., Coronado-Guti´errez, D., Valenzuela-Alcaraz, B., Bonet-
Carne, E., Eixarch, E., Crispi, F., Gratac´os, E.: Evaluation of deep convolutional
neural networks for automatic classification of common maternal fetal ultra-
sound planes. Scientific Reports 10(1), 10200 (2020) https://doi.org/10.1038/
s41598-020-67076-5
[33] Heuvel, T.L.A., Bruijn, D., Korte, C.L., Ginneken, B.: Automated measurement
16


## Page 17

of fetal head circumference using 2D ultrasound images. PLoS One 13(8), 0200412
(2018)
[34] Kiserud, T., Piaggio, G., Carroli, G., Widmer, M., Carvalho, J., Neerup Jensen,
L., Giordano, D., Cecatti, J.G., Abdel Aleem, H., Talegawkar, S.A., Benachi, A.,
Diemert, A., Tshefu Kitoto, A., Thinkhamrop, J., Lumbiganon, P., Tabor, A.,
Kriplani, A., Gonzalez Perez, R., Hecher, K., Hanson, M.A., G¨ulmezoglu, A.M.,
Platt, L.D.: The world health organization fetal growth charts: A multinational
longitudinal study of ultrasound biometric measurements and estimated fetal
weight. PLoS Med 14(1), 1002220 (2017)
[35] Lu, M.Y., Chen, B., Williamson, D.F.K., Chen, R.J., Zhao, M., Chow, A.K.,
Ikemura, K., Kim, A., Pouli, D., Patel, A., Soliman, A., Chen, C., Ding, T., Wang,
J.J., Gerber, G., Liang, I., Le, L.P., Parwani, A.V., Weishaupt, L.L., Mahmood,
F.: A multimodal generative ai copilot for human pathology. Nature 634(8033),
466–473 (2024) https://doi.org/10.1038/s41586-024-07618-3
[36] Zhang, J., Wang, K., Xu, R., Zhou, G., Hong, Y., Fang, X., Wu, Q., Zhang, Z.,
Wang, H.: Navid: Video-based vlm plans the next step for vision-and-language
navigation. Robotics: Science and Systems (2024)
[37] Munasinghe, S., Gani, H., Zhu, W., Cao, J., Xing, E., Khan, F.S., Khan, S.:
VideoGLaMM: A Large Multimodal Model for Pixel-Level Visual Grounding in
Videos (2024). https://arxiv.org/abs/2411.04923
[38] Rasheed, H., Maaz, M., Shaji, S., Shaker, A., Khan, S., Cholakkal, H., Anwer,
R.M., Xing, E., Yang, M.-H., Khan, F.S.: Glamm: Pixel grounding large mul-
timodal model. The IEEE/CVF Conference on Computer Vision and Pattern
Recognition (2024)
[39] Slimani, S., Hounka, S., Mahmoudi, A., Rehah, T., Laoudiyi, D., Saadi, H.,
Bouziyane, A., Lamrissi, A., Jalal, M., Bouhya, S., Akiki, M., Bouyakhf,
Y., Badaoui, B., Radgui, A., Mhlanga, M., Bouyakhf, E.H.: Fetal biome-
try and amniotic fluid volume assessment end-to-end automation using deep
learning. Nature Communications 14(1), 7047 (2023) https://doi.org/10.1038/
s41467-023-42438-5
[40] Wang, H., Wang, Z., Du, M., Yang, F., Zhang, Z., Ding, S., Mardziel, P., Hu, X.:
Score-cam: Score-weighted visual explanations for convolutional neural networks.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshops, pp. 24–25 (2020)
[41] Healy, J., McInnes, L.: Uniform manifold approximation and projection.
Nature Reviews Methods Primers 4(1), 82 (2024) https://doi.org/10.1038/
s43586-024-00363-x
[42] JaidedAI: EasyOCR. https://github.com/JaidedAI/EasyOCR (2025)
[43] Itseez: Open Source Computer Vision Library. https://github.com/itseez/opencv
(2015)
[44] Telea, A.: An image inpainting technique based on the fast marching method.
Journal of Graphics Tools 9(1), 23–34 (2004) https://doi.org/10.1080/10867651.
2004.10487596 https://doi.org/10.1080/10867651.2004.10487596
17


## Page 18

[45] Alasmawi, H., Bricker, L., Yaqub, M.: Fusc: Fetal ultrasound semantic clus-
tering of second-trimester scans using deep self-supervised learning. Ultra-
sound in Medicine & Biology 50(5), 703–711 (2024) https://doi.org/10.1016/j.
ultrasmedbio.2024.01.010
[46] Northcutt, C., Jiang, L., Chuang, I.: Confident learning: Estimating uncertainty
in dataset labels. Journal of Artificial Intelligence Research 70, 1373–1411 (2021)
[47] Ilharco, G., Wortsman, M., Wightman, R., Gordon, C., Carlini, N., Taori, R.,
Dave, A., Shankar, V., Namkoong, H., Miller, J., Hajishirzi, H., Farhadi, A.,
Schmidt, L.: OpenCLIP. https://doi.org/10.5281/zenodo.5143773 . If you use this
software, please cite it as below. https://doi.org/10.5281/zenodo.5143773
[48] Cherti, M., Beaumont, R., Wightman, R., Wortsman, M., Ilharco, G., Gordon,
C., Schuhmann, C., Schmidt, L., Jitsev, J.: Reproducible scaling laws for con-
trastive language-image learning. In: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pp. 2818–2829 (2023)
[49] RyanYip: Fine-tuned CLIP on PMC and ROCO Datasets for the General Medical
Domain (2023). https://huggingface.co/ryanyip7777/pmc vit l 14
[50] Hatamizadeh, A., Tang, Y., Nath, V., Yang, D., Myronenko, A., Landman,
B., Roth, H.R., Xu, D.: Unetr: Transformers for 3d medical image segmen-
tation. In: 2022 IEEE/CVF Winter Conference on Applications of Computer
Vision (WACV), pp. 1748–1758 (2022). https://doi.org/10.1109/WACV51458.
2022.00181
[51] Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T.,
Andreetto, M., Adam, H.: MobileNets: Efficient Convolutional Neural Networks
for Mobile Vision Applications (2017). https://arxiv.org/abs/1704.04861
[52] Buslaev, A., Iglovikov, V.I., Khvedchenya, E., Parinov, A., Druzhinin, M.,
Kalinin, A.A.: Albumentations: Fast and flexible image augmentations. Informa-
tion 11(2) (2020) https://doi.org/10.3390/info11020125
18


## Page 19

1
2
a. Dataset Curation
207,943
image-caption pairs
Routine
ultrasound
scan
Clinician-provided 
metadata
Ultrasound image
fetal brain
20 weeks and 4 days
0.43 mm/pixel
Pseudo-
caption
generator
2,092
image-caption pairs
Caption
Figure 7.13: Apical 
four-chamber view in 
diastole in grayscale (A) 
and color Doppler (B) in a 
normal fetus, using the 
simultaneous dual mode ...
Automatic
sub-captions
extraction
Textbook
Image-
caption
extraction
Ultrasound image
Manual
sub-captions
extraction
Ultrasound image at 20 
weeks and 4 days 
gestation focusing on the 
fetal brain, highlighting 
anatomical structures 
with a pixel spacing of 
0.43 mm/pixel.
Apical 
four-chamber view 
in diastole, in 
grayscale, of a 
normal fetus, using 
the simultaneous 
dual mode …
Images
Captions
Head
Heart
Abdomen
Spine
Lower Limb
Pelvic
Upper Limb
Proﬁle
Airway
Obstetric
102
103
104
105
Ultrasound 
image at 20 
weeks and 4 
days gestation 
focusing on the 
fetal brain, ...
1
Image
Encoder
Text
Encoder
b. FetalCLIP Pretraining
Apical 
four-chamber 
view in diastole 
in grayscale ...
2
Color Doppler 
demonstrates 
diastolic inﬂow 
...
N
...
...
...
Pretraining image–caption data
FetalCLIP model
Image embedding
Text embedding
Contrastive
Loss
Optimization
Pull
Push
d. Dataset Charactherization
I1
I2
IN
T1
T2
TN
Probability
....
Spine
Brain
Heart
Femur
True HC:
158.95 mm
Est. GA:
18w 6d
Dot product
12  16   20  24  28   32  36  40
Gestational age (weeks)
18w 6d
Plane
classiﬁcation
GA
estimation
CHD detection
Fetal structure
segmentation
Probability
Normal
Abnormal
Decoder
Linear
FetalCLIP
c. FetalCLIP Capability and Performance Evaluation
6.28 × 104
5.68 × 104
1.91 × 104
3.20 × 104
3.58 × 104
1.29 × 104
8.18 × 103
4.47 × 102
5.25 × 103
7.49 × 103
10 2
10 3
10 4
10 5
10 2
10 3
10 4
10 5
10 2
10 3
10 4
10 5
10 2
10 3
10 4
10 5
10 2
10 3
10 4
10 5
10 2
10 3
10 4
10 5
10 2
10 3
10 4
10 5
10 2
10 3
10 4
10 5
10 2
10 3
10 4
10 5
10 2
10 3
10 4
10 5
0
22
45
67
90
25
50
75
100
22
45
67
90
20
40
60
80
25
50
75
100
20
40
60
80
22
45
67
90
25
50
75
100
20
40
60
80
Gestational Age Estimation
6 Fetal
Ultrasound Views
Classiﬁcation
Brain
Sub-planes
Classiﬁcation
CHD
Detection
Head View Segmentation
Abdomen View
Segmentation
4-Chamber
View
Segmentation
5 Standard
Planes
Classiﬁcation
Brain
Sub-planes
Classiﬁcation
CLIP
BiomedCLIP
UniMed-CLIP
FetalCLIP
Zero-shot
Linear Probing
Decoder Probing
 H
ea
d 
 
   
   
   
   
H
ea
rt
 
 
 
 
 
 
 
 
 
  
   
   
   
   
   
Ai
rw
ay
 
   
   
  
O
bs
te
tr
ic
Proﬁle
Spine
 
 
 
 
 
   
   
 U
pp
er
 L
im
b 
   
   
  
Pe
lv
ic
   
   
   
   
   
   
Lo
w
er
 L
im
b 
   
   
   
   
   
   
   
   
A
bd
o
m
en
 
 
Foot
Tib
Legs
Femur
Thigh
Proﬁle
Trachea
Lung
Diaphragm
Contraction
Breech
Cephalic
Transverse
Fibroid
Ovary
Adnex
Cyst &
Subch
Cervix
Fluid
Placenta
Placenta fundi
Placenta anterior
Placenta posterior
Placenta right
Placenta tip
Scapula
Humerus
Upper limb
Hand or Arm
Fingers
Thumb
Heart
Aortic arch
3VT or 3VT
3TV
LVOT
4CH
RVOT
Thorax
Liver
Stomach
Kidney & Renal
Bowel
Cord insertion
2-Vessel cord
3-Vessel cord
Bladder
Spine
Cerebellum
Ventricular
Thalamic
Brain
Orbit
Eye
Face
Lense
Lips & Nose
Mouth
Septum
Neck
Mandible
Fig. 1: Overview of FetalCLIP development and performance. a, Dataset
curation of fetal ultrasound image-caption pairs used for the FetalCLIP pretraining.
The pretraining data was curated from two sources: (1) routine pregnancy ultra-
sound scans, comprising 207,943 images with corresponding LLM-generated pseudo
captions, which incorporate clinicians’ labels, gestational age, and pixel spacing; and
(2) 2,092 image-caption pairs derived from a fetal ultrasound textbook. b, FetalCLIP
pretraining step through contrastive learning, maximizing similarity between paired
image-captions while minimizing similarity to unrelated pairs. c, Schematic diagram
illustrating FetalCLIP’s capability and radar plot demonstrating FetalCLIP’s supe-
rior performance over existing vision-language foundation models across diverse fetal
ultrasound tasks, including fetal planes classification, congenital heart disease (CHD)
detection, and fetal structures segmentation on different views. d, Distribution of rou-
tine pregnancy ultrasound scan data, which constitutes the largest portion of the
FetalCLIP pretraining data.
19



### 图片

![2502.14807v3_page19_img1.png](images/2502.14807v3_page19_img1.png)

![2502.14807v3_page19_img2.png](images/2502.14807v3_page19_img2.png)

![2502.14807v3_page19_img3.png](images/2502.14807v3_page19_img3.png)

![2502.14807v3_page19_img4.png](images/2502.14807v3_page19_img4.png)

![2502.14807v3_page19_img5.png](images/2502.14807v3_page19_img5.png)

![2502.14807v3_page19_img6.png](images/2502.14807v3_page19_img6.png)

![2502.14807v3_page19_img7.png](images/2502.14807v3_page19_img7.png)

![2502.14807v3_page19_img8.png](images/2502.14807v3_page19_img8.png)

![2502.14807v3_page19_img9.png](images/2502.14807v3_page19_img9.png)

![2502.14807v3_page19_img10.png](images/2502.14807v3_page19_img10.png)

![2502.14807v3_page19_img11.png](images/2502.14807v3_page19_img11.png)

![2502.14807v3_page19_img12.png](images/2502.14807v3_page19_img12.png)

![2502.14807v3_page19_img13.png](images/2502.14807v3_page19_img13.png)

![2502.14807v3_page19_img14.png](images/2502.14807v3_page19_img14.png)

![2502.14807v3_page19_img15.png](images/2502.14807v3_page19_img15.png)

![2502.14807v3_page19_img16.png](images/2502.14807v3_page19_img16.png)

![2502.14807v3_page19_img17.png](images/2502.14807v3_page19_img17.png)

![2502.14807v3_page19_img18.png](images/2502.14807v3_page19_img18.png)

![2502.14807v3_page19_img19.png](images/2502.14807v3_page19_img19.png)

![2502.14807v3_page19_img20.png](images/2502.14807v3_page19_img20.png)

![2502.14807v3_page19_img21.png](images/2502.14807v3_page19_img21.png)

![2502.14807v3_page19_img22.png](images/2502.14807v3_page19_img22.png)

## Page 20

b. Zero-Shot Performance Across Fetal Planes
a. Zero-Shot Fetal Plane Classiﬁcation
d. Zero-Shot Gestational Age Estimation Performance
c. Zero-Shot Gestational Age Estimation
Probability
0.7
0.2
0.05
....
Image
Image Tokens
Abdomen
Brain
Heart
Femur
T1
I.T1
T2
I.T2
TN
I.TN
T3
I.T3
...
...
Text Encoder
Image Encoder
I
Abdomen prompt
Prompt generation
Fetal View
Abdomen
Brain
Heart
...
Femur
Brain prompt
Heart prompt
...
Femur prompt
0.03
Prompt
....
....
Dot product
Image
Image Tokens
T1
T2
TN
T3
...
Text Encoder
Image Encoder
I
12       16        20       24       28        32       36       40
Gestational age (weeks)
Ultrasound image at 20 weeks and 4 days 
gestation focusing on the fetal brain, 
highlighting anatomical structures with a 
pixel spacing of 0.43 mm/pixel.
I.T1
I.T2
I.TN
I.T3
...
CLIP ViT-L
BiomedCLIP
UniMed-CLIP
SonoNet
FetalCLIP
Abdomen
Heart
Femur
Cervix
Brain
Ventricular
Cerebellum
Thalamic
0
25
50
75
100
0
25
50
75
100
0
25
50
75
100
0
25
50
75
100
0
25
50
75
100
15
30
45
60
22
45
67
90
17
35
52
70
CLIP ViT-L
BiomedCLIP
UniMed-CLIP
SonoNet
FetalCLIP
0
20
40
60
80
100
Average F1-Score (%)
27.0%
46.6%
49.5%
69.9%
87.1%
Model Performance Comparison
Δ+17.2%
CLIP
FetalCLIP
100
150
200
250
300
350
True HC (mm)
100
125
150
175
200
225
250
275
Predicted GA (days)
UniMed-CLIP
100
150
200
250
300
350
BiomedCLIP
100
150
200
250
300
350
100
150
200
250
300
350
Valid prediction
Invalid prediction
Fig. 2: Zero-shot capabilities of FetalCLIP. a, Illustration of zero-shot fetal plane
classification. We leveraged an LLM to generate prompts for a set of predefined candi-
date planes (detailed in Extended Data Fig. 5). The predicted plane was determined
by identifying the highest similarity between the image embedding and prompt embed-
dings. b, Zero-shot performance in distinguishing five standard fetal planes and three
brain subplanes. FetalCLIP achieved the highest accuracy with an average F1 score
of 87.1%, outperforming the specialist model SonoNet by 17.2%. c, Illustration of
zero-shot GA estimation. A similarity map was computed between the image embed-
dings and prompts embeddings spanning 14 to 40 weeks of GA. We then subsequently
postprocessed the similarity map to predict GA. d, GA estimation performance of
visual-language foundation models. The blue points represent valid predictions, while
the red points indicate invalid predictions. The black line represents the 50th per-
centile of the quantile regression population, and the orange lines represent the 2.5th
and 97.5th percentiles of the population as provided by the WHO [34]. Unlike Fetal-
CLIP, other models demonstrated no ability to infer GA from fetal ultrasound head
images.
20



### 图片

![2502.14807v3_page20_img1.jpeg](images/2502.14807v3_page20_img1.jpeg)

![2502.14807v3_page20_img2.jpeg](images/2502.14807v3_page20_img2.jpeg)

![2502.14807v3_page20_img3.png](images/2502.14807v3_page20_img3.png)

![2502.14807v3_page20_img4.jpeg](images/2502.14807v3_page20_img4.jpeg)

![2502.14807v3_page20_img5.jpeg](images/2502.14807v3_page20_img5.jpeg)

![2502.14807v3_page20_img6.jpeg](images/2502.14807v3_page20_img6.jpeg)

![2502.14807v3_page20_img7.jpeg](images/2502.14807v3_page20_img7.jpeg)

![2502.14807v3_page20_img8.jpeg](images/2502.14807v3_page20_img8.jpeg)

![2502.14807v3_page20_img9.jpeg](images/2502.14807v3_page20_img9.jpeg)

![2502.14807v3_page20_img10.png](images/2502.14807v3_page20_img10.png)

![2502.14807v3_page20_img11.png](images/2502.14807v3_page20_img11.png)

![2502.14807v3_page20_img12.png](images/2502.14807v3_page20_img12.png)

## Page 21

a. Linear Probing for Fetal Plane Classiﬁcation
b. Fetal Plane Classiﬁcation
c. Fetal Brain Subplane Classiﬁcation
d. Linear Probing for CHD Detection
e. Fetal CHD Detection
f. Median Performance of Models for Fetal CHD Detection
Average
Other
Femur
Abdomen
Heart
Brain
Cervix
0.0
0.2
0.4
0.6
0.8
1.0
Test F1-Score
CLIP ViT-L
BiomedCLIP
UniMed-CLIP
FetalCLIP
Average
Ventricular
Thalamic
Cerebellum
0.0
0.2
0.4
0.6
0.8
1.0
Test F1-Score
CLIP ViT-L
BiomedCLIP
UniMed-CLIP
FetalCLIP
CLIP ViT-L
BiomedCLIP
UniMed-CLIP
FetalCLIP
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
Test AUROC
0.0
0.2
0.4
0.6
0.8
1.0
False Positive Rate (FPR)
0.0
0.2
0.4
0.6
0.8
1.0
True Positive Rate (TPR)
CLIP ViT-L | AUC: 0.70
BiomedCLIP | AUC: 0.66
UniMed-CLIP | AUC: 0.71
FetalCLIP | AUC: 0.80
Linear layer
Probability
0.08
0.05
0.12
0.70
....
Image Encoder
Ultrasound image
Image Tokens
Frame Embeddings 
(Z1 … Zn)
....
Abdomen
Brain
Heart
Femur
Linear layer
Probability
0.23
0.77
Image Encoder
Normal
Abnormal
C
...
...
𝑓n
...
𝑓3
𝑓2
𝑓1
Fig. 3: Linear probing for classification tasks. a, Schematic of linear probing for
classifying different fetal planes. The image encoder of a visual-language foundation
model was used to extract image embeddings, followed by a trainable linear layer for
classification. b-c, F1 scores in the testing set for fetal plane and brain subplane clas-
sification, from 5-fold cross-validations with five different seeds. The bars represent
the mean F1 scores, while the error bars indicate the standard deviation. d, Illustra-
tion of linear probing for CHD detection from an ultrasound clip. Embeddings were
extracted from each image in the clip and concatenated. A trainable linear layer was
then applied to leverage the combined embeddings for classification. e, AUROC com-
parisons for CHD detection across 5-fold cross-validations with 5 different seeds. f,
ROC curve for CHD prediction showing the median performance of each model.
21



### 图片

![2502.14807v3_page21_img1.png](images/2502.14807v3_page21_img1.png)

![2502.14807v3_page21_img2.png](images/2502.14807v3_page21_img2.png)

![2502.14807v3_page21_img3.png](images/2502.14807v3_page21_img3.png)

![2502.14807v3_page21_img4.png](images/2502.14807v3_page21_img4.png)

![2502.14807v3_page21_img5.png](images/2502.14807v3_page21_img5.png)

![2502.14807v3_page21_img6.png](images/2502.14807v3_page21_img6.png)

![2502.14807v3_page21_img7.png](images/2502.14807v3_page21_img7.png)

![2502.14807v3_page21_img8.png](images/2502.14807v3_page21_img8.png)

![2502.14807v3_page21_img9.png](images/2502.14807v3_page21_img9.png)

![2502.14807v3_page21_img10.png](images/2502.14807v3_page21_img10.png)

## Page 22

c. Segmentation Performance on Abdomen View
d. Segmentation Performance on 4-Chamber View
b. Average Segmentation Performance Across Fetal Views
a. Efﬁcient Image Encoder Adaptation for Fetal Structure Segmentation
Head View
0.9760
0.9765
0.9770
0.9775
0.9780
0.9785
0.9790
0.9795
0.9800
Abdomen View
0.74
0.76
0.78
0.80
0.82
0.84
4-Chamber View
0.66
0.68
0.70
0.72
0.74
Test DSC
CLIP ViT-L
BiomedCLIP
UniMed-CLIP
FetalCLIP
CLIP ViT-L
BiomedCLIP
UniMed-CLIP
FetalCLIP
CLIP ViT-L
BiomedCLIP
UniMed-CLIP
FetalCLIP
Stomach
Abdominal
Circumference
Spine
Average
0.5
0.6
0.7
0.8
0.9
Test DSC
CLIP ViT-L
BiomedCLIP
UniMed-CLIP
FetalCLIP
Thorax Aorta
Spine
Heart
LV
LA
RA
RV
IVS
Average
0.4
0.5
0.6
0.7
0.8
0.9
Test DSC
CLIP ViT-L
BiomedCLIP
UniMed-CLIP
FetalCLIP
Head View
Abdomen View
4-Chamber View
Z2NL/4
ZNL/4
Z3NL/4
ZNL
 Transformer block  2NL / 4
 Transformer block  3NL / 4
 Transformer block  NL 
Intermediate image features
Segmentation (Head View)
Segmentation (Abdomen View)
Segmentation (4-Chamber View)
Image encoder
Head View
decoder
Abdomen View
decoder
4-Chamber View
decoder
 Transformer block  NL / 4
...
...
...
...
Fig. 4: Segmentation of various fetal structures across different views. a,
Illustration of the efficient adaptation of an image encoder for segmenting fetal struc-
tures in different views. A lightweight decoder was developed to leverage intermediate
embeddings from the image encoder for segmentation. NL denotes the number of
transformer blocks in the image encoder, which is 12 for ViT-B and 24 for ViT-L,
respectively. b, Average segmentation performance across structures within each view
(head, abdomen, and 4-chamber) evaluated over 5-fold cross-validations with five dif-
ferent seeds. c-d, Dice Similarity Coefficient (DSC) for individual structures in the
abdomen view and 4-chamber view, respectively. LV, left ventricle; LA, left atrium;
RA, right atrium; RV, right ventricle; IVS, interventricular septum.
22



### 图片

![2502.14807v3_page22_img1.png](images/2502.14807v3_page22_img1.png)

![2502.14807v3_page22_img2.png](images/2502.14807v3_page22_img2.png)

![2502.14807v3_page22_img3.png](images/2502.14807v3_page22_img3.png)

![2502.14807v3_page22_img4.jpeg](images/2502.14807v3_page22_img4.jpeg)

![2502.14807v3_page22_img5.jpeg](images/2502.14807v3_page22_img5.jpeg)

![2502.14807v3_page22_img6.jpeg](images/2502.14807v3_page22_img6.jpeg)

## Page 23

b. CAMs for Gestational Age Estimation
a. CAMs for Fetal View Classiﬁcation
d. UMAP of FetalCLIP (Diverse Views)
c. UMAP of FetalCLIP Image Embeddings
Other
Expert-veriﬁed cluster
Abdomen
Brain
Cervix
Femur
Heart
Abdomen
Brain
Cervix
Femur
Heart
Cluster 1: Heart
Cluster 2: Vascular
Cluster 3: Extremities
Cluster 4: Orbit
Cluster 5: Lips & Nose
Cluster 6: Profie
Cluster 7: Diaphragm
Cluster 8: Spine
Cluster 9: Table
Abdomen
Femur
Brain
Cervix
Heart
Ventricular
Cerebellum
Thalamic
Thalamic
Cerebellum
Ventricular
1
2
3
4
5
6
7
8
9
True HC: 158.95 mm
True HC: 162.33 mm
True HC: 180.89 mm
Est. GA: 18w 6d
Est. GA: 18w 6d
Est. GA: 21w 5d
Fig. 5: FetalCLIP interpretation studies. a, Class activation maps (CAMs)
of FetalCLIP highlighting important regions for predicting fetal ultrasound views.
FetalCLIP effectively emphasized important structures when determining anatomi-
cal views. b, CAMs of FetalCLIP when estimating gestational age, highlighting brain
structures and surrounding head circumference regions. c, UMAP visualization of
FetalCLIP image embeddings, corresponding to five standard fetal planes and three
brain subviews. d, Image embeddings of diverse fetal views. Images containing similar
information were mapped into close proximity. We manually inspected several points
in each cluster belonging to the ”Other” class to determine the dominant views within
each cluster, which were then verified by clinicians.
23



### 图片

![2502.14807v3_page23_img1.jpeg](images/2502.14807v3_page23_img1.jpeg)

![2502.14807v3_page23_img2.png](images/2502.14807v3_page23_img2.png)

![2502.14807v3_page23_img3.jpeg](images/2502.14807v3_page23_img3.jpeg)

![2502.14807v3_page23_img4.png](images/2502.14807v3_page23_img4.png)

![2502.14807v3_page23_img5.jpeg](images/2502.14807v3_page23_img5.jpeg)

![2502.14807v3_page23_img6.jpeg](images/2502.14807v3_page23_img6.jpeg)

![2502.14807v3_page23_img7.jpeg](images/2502.14807v3_page23_img7.jpeg)

![2502.14807v3_page23_img8.png](images/2502.14807v3_page23_img8.png)

![2502.14807v3_page23_img9.jpeg](images/2502.14807v3_page23_img9.jpeg)

![2502.14807v3_page23_img10.jpeg](images/2502.14807v3_page23_img10.jpeg)

![2502.14807v3_page23_img11.jpeg](images/2502.14807v3_page23_img11.jpeg)

![2502.14807v3_page23_img12.png](images/2502.14807v3_page23_img12.png)

![2502.14807v3_page23_img13.jpeg](images/2502.14807v3_page23_img13.jpeg)

![2502.14807v3_page23_img14.png](images/2502.14807v3_page23_img14.png)

![2502.14807v3_page23_img15.png](images/2502.14807v3_page23_img15.png)

![2502.14807v3_page23_img16.png](images/2502.14807v3_page23_img16.png)

![2502.14807v3_page23_img17.jpeg](images/2502.14807v3_page23_img17.jpeg)

![2502.14807v3_page23_img18.png](images/2502.14807v3_page23_img18.png)

![2502.14807v3_page23_img19.png](images/2502.14807v3_page23_img19.png)

![2502.14807v3_page23_img20.jpeg](images/2502.14807v3_page23_img20.jpeg)

![2502.14807v3_page23_img21.jpeg](images/2502.14807v3_page23_img21.jpeg)

![2502.14807v3_page23_img22.png](images/2502.14807v3_page23_img22.png)

![2502.14807v3_page23_img23.png](images/2502.14807v3_page23_img23.png)

![2502.14807v3_page23_img24.jpeg](images/2502.14807v3_page23_img24.jpeg)

![2502.14807v3_page23_img25.jpeg](images/2502.14807v3_page23_img25.jpeg)

![2502.14807v3_page23_img26.png](images/2502.14807v3_page23_img26.png)

![2502.14807v3_page23_img27.jpeg](images/2502.14807v3_page23_img27.jpeg)

![2502.14807v3_page23_img28.jpeg](images/2502.14807v3_page23_img28.jpeg)

![2502.14807v3_page23_img29.jpeg](images/2502.14807v3_page23_img29.jpeg)

![2502.14807v3_page23_img30.jpeg](images/2502.14807v3_page23_img30.jpeg)

![2502.14807v3_page23_img31.jpeg](images/2502.14807v3_page23_img31.jpeg)

![2502.14807v3_page23_img32.jpeg](images/2502.14807v3_page23_img32.jpeg)

![2502.14807v3_page23_img33.jpeg](images/2502.14807v3_page23_img33.jpeg)

![2502.14807v3_page23_img34.jpeg](images/2502.14807v3_page23_img34.jpeg)

![2502.14807v3_page23_img35.png](images/2502.14807v3_page23_img35.png)

![2502.14807v3_page23_img36.jpeg](images/2502.14807v3_page23_img36.jpeg)

![2502.14807v3_page23_img37.png](images/2502.14807v3_page23_img37.png)

![2502.14807v3_page23_img38.jpeg](images/2502.14807v3_page23_img38.jpeg)

![2502.14807v3_page23_img39.png](images/2502.14807v3_page23_img39.png)

## Page 24

Extended Data Fig. 1: Confusion matrices for five standard fetal planes
and three brain subviews classifications. a-c, represent the confusion matrices
of FetalCLIP, SonoNet, and UniMed-CLIP, respectively.
24



### 图片

![2502.14807v3_page24_img1.png](images/2502.14807v3_page24_img1.png)

## Page 25

Extended Data Fig. 2: Examples of various image views from the private
hospital dataset. a, Representative examples of standard views from the fetal ultra-
sound dataset, showcasing diverse anatomical planes such as 4CH, Femur, Kidney, and
Transcerebellum. b, Examples of mislabeled samples detected by Confident Learning.
c, Ultrasound images containing multiple clinician labels.
25



### 图片

![2502.14807v3_page25_img1.jpeg](images/2502.14807v3_page25_img1.jpeg)

## Page 26

Extended Data Fig. 3: Data-efficient transfer learning for downstream
tasks. The image encoder was kept frozen during these experiments. Data from N
patients were used for training, with an additional N patients for the validation set,
collectively defined as the support set, while the final performance was evaluated on
an independent test set. Experiments were conducted across five support sets using
five different random seeds, yielding a total of 25 results. a-b, Classification perfor-
mance for six fetal planes and three brain subplanes, respectively. c–e, Segmentation
performance on the head, abdomen, and four-chamber views, respectively.
26



### 图片

![2502.14807v3_page26_img1.png](images/2502.14807v3_page26_img1.png)

## Page 27

"You are a helpful medical
assistant."
System Message
"Write a medical caption for a fetal ultrasound image focusing on the {label}. Do not add any unconfirmed information. Ensure
the caption includes a phrase like 'showing,' 'highlighting,' or 'focusing.'. Maintain a neutral tone, avoiding any mention of
abnormalities or clinical concerns.  Indicate a gestational age of {'{weeks}'} weeks and {'{day}'} days, with a pixel spacing of
{'{pixel_spacing}'} mm/pixel. Keep the caption under 50 words and 1 sentence."
Template Prompt Message
"Write a medical caption for a fetal ultrasound image focusing on the
abdomen. Do not add any unconfirmed information. ... "
Example Prompt Input:
"Fetal ultrasound at {weeks} weeks and {day} days gestation, focusing on the
abdominal area, highlighting structural development with a pixel spacing of
{pixel_spacing} mm/pixel."
"Fetal ultrasound image at {weeks} weeks and {day} days gestational age,
focusing on the abdomen with a pixel spacing of {pixel_spacing} mm/pixel."
...
Example GPT-4o Outputs:
"Write a medical caption for a fetal ultrasound image focusing on the heart 4-ch.
Do not add any unconfirmed information. ... "
Example Prompt Input:
"Ultrasound image highlighting the fetal heart in a 4-chamber view
at {weeks} weeks and {day} days gestational age, with a pixel spacing
of {pixel_spacing} mm/pixel."
"Ultrasound image of a fetal heart in the four-chamber view, focusing on the
anatomical structures at {weeks} weeks and {day} days of gestation, with a
pixel spacing of {pixel_spacing} mm/pixel."
...
Example GPT-4o Outputs:
"You are a helpful medical
assistant."
System Message
"Write a medical caption for a fetal ultrasound image focusing on the {label}. Some information may be inferred from the
aforementioned structure(s) but do not add any unconfirmed information. Ensure the caption includes a phrase like 'showing,'
'highlighting,' or 'focusing.'. Maintain a neutral tone, avoiding any mention of abnormalities or clinical concerns. Indicate a
gestational age of {'{weeks}'} weeks and {'{day}'} days, with a pixel spacing of {'{pixel_spacing}'} mm/pixel. Keep the caption
under 70 words."
Template Prompt Message
"Write a medical caption for a fetal ultrasound image focusing on the cervix, bladder, and head. Some information may be inferred ... "
Example Prompt Input:
"Ultrasound image at {weeks} weeks and {day} days gestational age, highlighting the fetal head, maternal bladder, and cervical region. The clear visualization of
these structures within the imaging field allows for assessment of fetal positioning and proximity to the cervix. The pixel spacing is {pixel_spacing} mm/pixel,
providing precise detail to support the ongoing monitoring of fetal development."
...
Example GPT-4o Outputs:
a
b
"Revise the following caption related to a fetal ultrasound image to create a fully
self-contained, independent description. Retain the original information but
remove any references to visual markers or annotations such as arrows, stars,
or symbols. Avoid mentioning specific figure labels (e.g., Figure A or first
figure). Present the caption as plain text, without any introductory comments or
formatting labels.\n\n"{caption}""
Template Prompt: No Subcaption
"Given the caption of an image with {num_sub_captions} sub-figures related
to fetal ultrasound, separate and extract each sub-caption so that it is an
independent, self-contained description. Ensure each sub-caption provides
context and details about the respective sub-figure, but omit any references to
indicators or annotations in the image (e.g., arrows, stars, or symbols) and
avoid mentioning labels such as (subfigure A) and (first subfigure). Format the
subcaptions as a text separated by symbol &&, without any additional
comments or labels.\n\n"{caption}""
Template Prompt: With Subcaptions
c
"You are a helpful medical assistant."
System Message
Extended Data Fig. 4: Prompts used to generate captions for routine clin-
ical scan data and image-caption pairs derived from a textbook. a, Prompts
for generating caption templates for the 12 standard views, four cardiac subviews, and
brain subplanes. b, Prompts for generating caption templates for other diverse labels,
including images with multiple views. c, Prompts for preprocessing captions to gen-
erate subcaptions for datasets derived from image-caption pairs from a textbook.
27


## Page 28

"Generate 5 typical captions for CLIP zero-shot fetal ultrasound
images for the following classes: abdomen, brain, femur, heart,
kidney, lips_nose, profile_patient, spine, and cervix."
Generating Typical Zero-Shot
Prompts for CLIP
"Revise the below prompts by removing the {pixel_spacing} and gestational age. Please modify
the style so that it will work for the CLIP model (medical)
{
    "abdomen": [
        "Fetal ultrasound at {weeks} weeks and {day} days gestation, focusing on the abdominal
area, highlighting structural development with a pixel spacing of {pixel_spacing} mm/pixel.",
        ...
    ],
    "brain": [
        "Ultrasound image at {weeks} weeks and {day} days gestation focusing on the fetal brain,
highlighting anatomical structures with a pixel spacing of {pixel_spacing} mm/pixel.",
        ...
    ],
    ...
}
"
Generating Prompts for Inference from
Training Caption Templates
{
  "abdomen": [
    "Ultrasound image of the fetal abdomen with visible organs.",
    "Sonogram showing the fetal abdomen in a clear view.",
    ...
  ],
  "brain": [
    "Ultrasound image of the fetal brain with visible structures.",
    "Sonogram showing the fetal brain and its details.",
    ...
  ],
  ...
}
Example GPT-4o Outputs:
{
  "abdomen": [
    "Ultrasound image focusing on the fetal abdominal area, highlighting structural development.",
    "Detailed ultrasound highlighting the fetal abdomen, emphasizing anatomical structures.",
    ...
  ],
  "brain": [
    "Ultrasound image focusing on the fetal brain, highlighting key anatomical features.",
    "Detailed ultrasound scan of the developing fetal brain, showcasing structural highlights.",
     ...
  ],
  ...
}
Example GPT-4o Outputs:
Extended Data Fig. 5: Prompts for inference. Five text prompts were designed
for each target class using GPT-4o. We generated two types of inference prompts:
typical prompts for CLIP models and prompts incorporating information from caption
templates of routine pregnancy scan data.
28


## Page 29

Extended Data Table 1. Dataset details and training protocols for downstream
tasks. Two public datasets (Planes DB [32], HC18 [33]) and three private datasets were
utilized for benchmarking. Two experimental settings were investigated: full data and
data-efficient training. Image augmentation was performed offline and implemented using
Albumentations [52].
CHD 
Detection
Full 
Data
Few 
Data
Full 
Data
Few 
Data
Full Data
Full 
Data
Few 
Data
Full 
Data
Few 
Data
Full 
Data
Few 
Data
Data Source
Private
Anatomical
Classes or
Structures
Normal,
Abnormal
# of Images
(train - test)
333 - 85
# of Patients
(train - test)
100 - 26
Loss function
Epochs
50
100
50
100
5
100
100
100
100
100
100
Batch Size
64
16
64
16
32
8
8
16
16
16
8
Optimizer
Learning Rate
3e-4
3e-4
3e-4
1e-4
1e-4
3e-4
3e-4
3e-4
3e-4
3e-4
3e-4
Weight Decay
0.01
0.01
0.01
0.01
0.2
0.01
0.01
0.01
0.01
0.01
0.01
# Augmentations
per Image
5
10
5
10
15
5
10
5
10
5
10
Augmentation
Pipeline
87 - 24
HC18
Planes DB
Abdominal View
Segmentation Tasks
Dataset
Cross-entropy loss
Dice loss
663 - 161
Head View
Hyperparameters
Classification Tasks
Transcerebellum,
Transthalamic,
Transventricular
1543 - 1406
524 - 536
Brain Sub-view 
Classification
Private
Abdomen,
Stomach,
Spine
Head
826 - 198
Augmentation
View 
Classification
Planes DB
Abdomen, Brain,
Cervix, Femur,
Thorax, Other
7129 - 5271
896 - 896
A.Compose([
A.ColorJitter(0.2, 0.2, 0.2, 0.2, p=0.5),
A.CLAHE(p=0.5),
A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.0, rotate_limit=20,
interpolation=cv2.INTER_LINEAR,
border_mode=cv2.BORDER_CONSTANT, value=0, p=1.)])
AdamW
29 - 8
4-Chamber View
Private
Thorax, Spine,
Aorta, Heart, LV,
RV, LA, RA, IVS
296 - 59
58 - 15
29

